{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import components\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WMT14 dataset for German-English translation\n",
    "dataset = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 4508785\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3003\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.',\n",
       "  'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a very small segment for experimentation\n",
    "# Take a small subset for experimentation\n",
    "small_train_dataset = dataset['train'].select(range(20))\n",
    "small_val_dataset = dataset['validation'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are following the original `Attention is all you need paper` we will use Byte-Pair Encoding\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"bpe_tokenizer/vocab.json\",\n",
    "    \"bpe_tokenizer/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[789, 423, 328, 3010, 18]\n",
      "['Das', 'Ġist', 'Ġein', 'ĠBeispiel']\n",
      "2\n",
      "Das ist ein Beispiel.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Das ist ein Beispiel.\").ids)\n",
    "\n",
    "print([tokenizer.id_to_token(token) for token in tokenizer.encode(\"Das ist ein Beispiel\").ids])\n",
    "# Should return something like ['<s>', 'Das', 'ist', 'ein', 'Beispiel', '</s>']\n",
    "\n",
    "print(tokenizer.token_to_id(\"</s>\"))\n",
    "# Should return a valid token ID for '</s>'\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(\"Das ist ein Beispiel.\").ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = tokenizer.token_to_id('<pad>')\n",
    "BOS_TOKEN_ID = tokenizer.token_to_id('<s>')\n",
    "EOS_TOKEN_ID = tokenizer.token_to_id('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, bos_token_id: int = BOS_TOKEN_ID, eos_token_id: int = EOS_TOKEN_ID ,pad_token_id:int = PAD_TOKEN_ID, max_length: int = 512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.bos = bos_token_id\n",
    "        self.eos = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.dataset[idx]['translation']['de']\n",
    "        tgt_sentence = self.dataset[idx]['translation']['en']\n",
    "\n",
    "        # tokenize the source and target\n",
    "        src_tokens = self.tokenizer.encode(src_sentence).ids\n",
    "        tgt_tokens = self.tokenizer.encode(tgt_sentence).ids\n",
    "\n",
    "        # pad and truncate\n",
    "        src_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(src_tokens)))\n",
    "        tgt_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(tgt_tokens)))\n",
    "\n",
    "        # # create attention masks\n",
    "        # src_mask = (src_tokens != self.pad_token_id).int()\n",
    "        # tgt_mask = (src_tokens != self.pad_token_id).int()\n",
    "\n",
    "        # # create look ahead mask\n",
    "        # look_ahead_mask = self.create_causal_mask(len(tgt_tokens))\n",
    "\n",
    "\n",
    "        return {\n",
    "            'src_sentence': src_sentence, \n",
    "            'tgt_sentence': tgt_sentence, \n",
    "            'src_tokens': src_tokens,\n",
    "            'tgt_tokens': tgt_tokens,\n",
    "            # 'src_mask': src_mask,\n",
    "            # 'tgt_mask': tgt_mask,\n",
    "            # 'look_ahead_mask': look_ahead_mask,\n",
    "            # 'combined_mask': tgt_mask & look_ahead_mask\n",
    "        }\n",
    "\n",
    "    def pad_and_truncate(self, tokens):\n",
    "        if len(tokens) < self.max_length:\n",
    "            tokens = tokens + [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def add_special_tokens(self, tokens):\n",
    "        return [self.bos] + tokens + [self.eos]\n",
    "\n",
    "    def create_causal_mask(self, size):\n",
    "        # create an lower triangular matrix for the purposes of look ahead masking\n",
    "        return torch.tril(torch.ones(size, size)).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TranslationDataset at 0x17f522390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_translation_ds = TranslationDataset(small_train_dataset, tokenizer=tokenizer, pad_token_id=PAD_TOKEN_ID, max_length=30)\n",
    "small_translation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_sentence': 'Wiederaufnahme der Sitzungsperiode',\n",
       " 'tgt_sentence': 'Resumption of the session',\n",
       " 'src_tokens': tensor([    0, 23062, 17719,   319, 26699,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'tgt_tokens': tensor([    0,  8859, 27958,   304,   280,  9974,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_translation_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function for handling masks\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    return torch.tril(torch.ones(attn_shape)).type(torch.uint8)  # Shape: (1, seq_length, seq_length)\n",
    "\n",
    "def create_std_mask(tgt, pad_token_id = PAD_TOKEN_ID):\n",
    "    tgt_mask = (tgt != pad_token_id).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & create_causal_mask(tgt.size(-1))\n",
    "    return tgt_mask\n",
    "    \n",
    "def collate_fn(batch, pad_token_id = PAD_TOKEN_ID):\n",
    "    src_batch = torch.stack([item['src_tokens'] for item in batch])\n",
    "    tgt_batch = torch.stack([item['tgt_tokens'] for item in batch])\n",
    "\n",
    "    # create source masks\n",
    "    src_mask = (src_batch != pad_token_id).unsqueeze(-2).int() # shape: (bs, seq_length, 1)\n",
    "    tgt = tgt_batch[:, :-1]\n",
    "    tgt_y = tgt_batch[:, 1:]\n",
    "    tgt_mask = create_std_mask(tgt, pad_token_id=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'src_tokens': src_batch,\n",
    "        'tgt_input': tgt, \n",
    "        'tgt_output': tgt_y,\n",
    "        'src_mask': src_mask, \n",
    "        'tgt_mask': tgt_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tokens: torch.Size([4, 30])\n",
      "Target tokens: torch.Size([4, 29])\n",
      "Target output tokens: torch.Size([4, 29])\n",
      "Source mask: torch.Size([4, 1, 30])\n",
      "Target mask: torch.Size([4, 29, 29])\n"
     ]
    }
   ],
   "source": [
    "small_dl = DataLoader(small_translation_ds, collate_fn=collate_fn, batch_size=4)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each layer step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def scaled_dpa(query, key, value, mask=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Implements scaled dot product attention.\n",
    "    Args:\n",
    "        query: (batch_size, seq_length, dim_k)\n",
    "        key: (batch_size, seq_length, dim_k)\n",
    "        value: (batch_size, seq_length, dim_v)\n",
    "        mask: (batch_size, seq_length) or None\n",
    "        verbose: Boolean default False\n",
    "    Returns:\n",
    "        attention_output: (batch_size, seq_length, dim_v)\n",
    "        attention_weights: (batch_size, seq_length, seq_length)\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)# (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Scores shape: {scores.shape}\")\n",
    "    \n",
    "    # apply the mask if necessary\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    \n",
    "    # apply softmax to get attention_weights\n",
    "    attention_weights = F.softmax(scores, dim=-1) # (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Attention output shape: {output.shape}\")\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0564, 0.7034, 0.8226, 0.9724, 0.3710],\n",
      "         [0.0805, 0.8497, 0.6111, 0.0341, 0.6167],\n",
      "         [0.9636, 0.0938, 0.4513, 0.9342, 0.6937],\n",
      "         [0.2900, 0.0510, 0.8669, 0.6969, 0.1171],\n",
      "         [0.0601, 0.2315, 0.6242, 0.9649, 0.2666]],\n",
      "\n",
      "        [[0.4661, 0.3682, 0.9229, 0.9852, 0.7550],\n",
      "         [0.9225, 0.8854, 0.4957, 0.1771, 0.0352],\n",
      "         [0.9176, 0.7769, 0.0342, 0.4605, 0.5081],\n",
      "         [0.2047, 0.8456, 0.0671, 0.6140, 0.6816],\n",
      "         [0.5452, 0.2121, 0.8647, 0.9185, 0.3518]],\n",
      "\n",
      "        [[0.0299, 0.5612, 0.8721, 0.6243, 0.2046],\n",
      "         [0.0583, 0.3612, 0.5550, 0.9397, 0.2131],\n",
      "         [0.6086, 0.9861, 0.0937, 0.9850, 0.9747],\n",
      "         [0.9554, 0.3499, 0.2822, 0.2843, 0.0721],\n",
      "         [0.8433, 0.2051, 0.2551, 0.5208, 0.3766]]])\n",
      "tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "torch.Size([3, 1, 5])\n",
      "tensor([[[0.0564, 0.7034, 0.8226,   -inf,   -inf],\n",
      "         [0.0805, 0.8497, 0.6111,   -inf,   -inf],\n",
      "         [0.9636, 0.0938, 0.4513,   -inf,   -inf],\n",
      "         [0.2900, 0.0510, 0.8669,   -inf,   -inf],\n",
      "         [0.0601, 0.2315, 0.6242,   -inf,   -inf]],\n",
      "\n",
      "        [[0.4661, 0.3682,   -inf,   -inf,   -inf],\n",
      "         [0.9225, 0.8854,   -inf,   -inf,   -inf],\n",
      "         [0.9176, 0.7769,   -inf,   -inf,   -inf],\n",
      "         [0.2047, 0.8456,   -inf,   -inf,   -inf],\n",
      "         [0.5452, 0.2121,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.0299, 0.5612, 0.8721, 0.6243, 0.2046],\n",
      "         [0.0583, 0.3612, 0.5550, 0.9397, 0.2131],\n",
      "         [0.6086, 0.9861, 0.0937, 0.9850, 0.9747],\n",
      "         [0.9554, 0.3499, 0.2822, 0.2843, 0.0721],\n",
      "         [0.8433, 0.2051, 0.2551, 0.5208, 0.3766]]])\n"
     ]
    }
   ],
   "source": [
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "\n",
    "# example scores\n",
    "scores = torch.rand(batch_size, seq_length, seq_length)\n",
    "print(scores)\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "print(mask)\n",
    "\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask.shape)\n",
    "\n",
    "scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([3, 5, 4])\n",
      "Mask shape: torch.Size([3, 1, 5])\n",
      "Scores shape: torch.Size([3, 5, 5])\n",
      "Attention weights shape: torch.Size([3, 5, 5])\n",
      "Attention output shape: torch.Size([3, 5, 4])\n",
      "Attention Output:\n",
      " tensor([[[0.3008, 0.4424, 0.4457, 0.4867],\n",
      "         [0.3205, 0.4076, 0.4672, 0.4865],\n",
      "         [0.3131, 0.4118, 0.4660, 0.4882],\n",
      "         [0.3042, 0.4358, 0.4499, 0.4868],\n",
      "         [0.3049, 0.4364, 0.4492, 0.4865]],\n",
      "\n",
      "        [[0.3656, 0.3903, 0.8493, 0.4133],\n",
      "         [0.3650, 0.3868, 0.8480, 0.4115],\n",
      "         [0.3659, 0.3916, 0.8498, 0.4140],\n",
      "         [0.3670, 0.3975, 0.8520, 0.4170],\n",
      "         [0.3650, 0.3869, 0.8481, 0.4116]],\n",
      "\n",
      "        [[0.3589, 0.2890, 0.3432, 0.2227],\n",
      "         [0.3632, 0.2925, 0.3509, 0.2238],\n",
      "         [0.3517, 0.2864, 0.3403, 0.2223],\n",
      "         [0.3563, 0.2923, 0.3405, 0.2157],\n",
      "         [0.3641, 0.2926, 0.3490, 0.2219]]], device='mps:0')\n",
      "Attention Weights:\n",
      " tensor([[[0.3217, 0.3063, 0.3720, 0.0000, 0.0000],\n",
      "         [0.2716, 0.3372, 0.3912, 0.0000, 0.0000],\n",
      "         [0.2798, 0.3235, 0.3968, 0.0000, 0.0000],\n",
      "         [0.3123, 0.3115, 0.3762, 0.0000, 0.0000],\n",
      "         [0.3127, 0.3129, 0.3743, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5215, 0.4785, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5350, 0.4650, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5164, 0.4836, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4934, 0.5066, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5348, 0.4652, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2336, 0.1831, 0.1989, 0.1976, 0.1868],\n",
      "         [0.2373, 0.1883, 0.2097, 0.2001, 0.1646],\n",
      "         [0.2340, 0.1725, 0.1914, 0.2184, 0.1837],\n",
      "         [0.2179, 0.1927, 0.2013, 0.1922, 0.1958],\n",
      "         [0.2313, 0.1932, 0.2106, 0.1820, 0.1830]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# test scaled dpa\n",
    "# Example of how to use scaled_dpa with random tensors\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "mask = mask.unsqueeze(1).to(device)\n",
    "\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Test scaled_dpa\n",
    "output, attention_weights = scaled_dpa(query, key, value, mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: torch.Size([1, 5, 5])\n",
      "Attention weights shape: torch.Size([1, 1, 5, 5])\n",
      "Attention output shape: torch.Size([1, 1, 5, 4])\n",
      "Attention Output:\n",
      " tensor([[[[0.0541, 0.0825, 0.4685, 0.6847],\n",
      "          [0.3995, 0.3328, 0.6036, 0.4355],\n",
      "          [0.2561, 0.3375, 0.4431, 0.3427],\n",
      "          [0.2636, 0.3385, 0.4502, 0.3452],\n",
      "          [0.2579, 0.3349, 0.4475, 0.3484]]]], device='mps:0')\n",
      "Attention Weights:\n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4876, 0.5124, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3333, 0.3085, 0.3582, 0.0000, 0.0000],\n",
      "          [0.3370, 0.3192, 0.3438, 0.0000, 0.0000],\n",
      "          [0.3437, 0.3108, 0.3455, 0.0000, 0.0000]]]], device='mps:0')\n",
      "Padding Mask:\n",
      " tensor([[[[ True,  True,  True, False, False]]]], device='mps:0')\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], device='mps:0', dtype=torch.uint8)\n",
      "Combined Mask:\n",
      " tensor([[[[1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 0, 0]]]], device='mps:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# testing with mask\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a padding mask (1 for valid tokens, 0 for padding tokens).\n",
    "    seq: Tensor of shape (batch_size, seq_length)\n",
    "    \"\"\"\n",
    "    return (seq != 0).unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(size, size)).type(torch.uint8)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Test scaled_dpa with padding and causal masks\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 1\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0, 0]]).to(device)  # Example with 2 padding tokens\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (for testing both padding and causal masking together)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0).to(device)\n",
    "\n",
    "# Test scaled_dpa with the mask\n",
    "output, attention_weights = scaled_dpa(query, key, value, combined_mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout=0.1, verbose=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Num heads: {num_heads}\")\n",
    "            print(f\"Embedding dimension: {d_model}\")\n",
    "            print(f\"per head dimension: {self.d_k}\")\n",
    "    \n",
    "        # linear layers to project the inputs to query, key, and value\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout) \n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query shape is bs, seq_length, d_model\n",
    "        # key shape is bs, seq_length, d_model\n",
    "        # value shape is bs, d_model, d_model\n",
    "        batch_size = query.size(0)\n",
    "        seq_length = query.size(1)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # Same mask applied to all heads. \n",
    "\n",
    "        if self.verbose and mask is not None:\n",
    "            print(f\"Mask shape (after unsqueezing at 1): {mask.shape}\")\n",
    "\n",
    "        # apply linear layers\n",
    "        query = self.query_linear(query)   # shape bs, seq_length, d_model\n",
    "        key = self.key_linear(key) #shape: bs, seq_length, d_model\n",
    "        value = self.value_linear(value) # shape: bs, d_model, d_model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Query shape: {query.shape}\")\n",
    "            print(f\"Key shape: {key.shape}\")\n",
    "            print(f\"Value shape: {value.shape}\")\n",
    "        \n",
    "        # reshape and split into multiple heads\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) #(bs, num_heads, seq_length, d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Shapes after projections for query, key, value...\")\n",
    "            print(f\"{query.shape}, {key.shape}, {value.shape}\")\n",
    "\n",
    "        attn_output, attn_weights = scaled_dpa(query, key, value, mask, verbose = self.verbose)\n",
    "\n",
    "        # we've separated the query key and value into separate heads and then computed the scaled dot-product attention for each head.\n",
    "        # Now we must put them back together. \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Attention output shape after concat: {attn_output.shape}\")\n",
    "\n",
    "        # apply the final linear layer transformation\n",
    "        output = self.output_linear(attn_output)\n",
    "        if self.verbose:\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "unchanged query: tensor([[[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
      "         [17, 18, 19, 20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29, 30, 31, 32]]], device='mps:0')\n",
      "prior to transpose query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]],\n",
      "\n",
      "         [[17, 18, 19, 20],\n",
      "          [21, 22, 23, 24]],\n",
      "\n",
      "         [[25, 26, 27, 28],\n",
      "          [29, 30, 31, 32]]]], device='mps:0')\n",
      "transposed query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [17, 18, 19, 20],\n",
      "          [25, 26, 27, 28]],\n",
      "\n",
      "         [[ 5,  6,  7,  8],\n",
      "          [13, 14, 15, 16],\n",
      "          [21, 22, 23, 24],\n",
      "          [29, 30, 31, 32]]]], device='mps:0')\n",
      "torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# visualizing how the transpsoe works\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model).to(device)\n",
    "print(query.shape)\n",
    "print(f\"unchanged query: {query}\")\n",
    "\n",
    "query = query.view(batch_size, -1, num_heads, d_k)\n",
    "print(f\"prior to transpose query: {query}\")\n",
    "\n",
    "query = query.transpose(1, 2)\n",
    "print(f\"transposed query: {query}\")\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly reshaping query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]],\n",
      "\n",
      "         [[17, 18, 19, 20],\n",
      "          [21, 22, 23, 24],\n",
      "          [25, 26, 27, 28],\n",
      "          [29, 30, 31, 32]]]])\n"
     ]
    }
   ],
   "source": [
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model)\n",
    "\n",
    "query = query.view(batch_size, num_heads, -1, d_k)\n",
    "print(f\"Directly reshaping query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 8\n",
      "Embedding dimension: 64\n",
      "per head dimension: 8\n",
      "Query shape: torch.Size([1, 5, 64])\n",
      "Key shape: torch.Size([1, 5, 64])\n",
      "Value shape: torch.Size([1, 5, 64])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 8, 5, 8]), torch.Size([1, 8, 5, 8]), torch.Size([1, 8, 5, 8])\n",
      "Scores shape: torch.Size([1, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([1, 8, 5, 5])\n",
      "Attention output shape: torch.Size([1, 8, 5, 8])\n",
      "Attention output shape after concat: torch.Size([1, 5, 64])\n",
      "Output shape: torch.Size([1, 5, 64])\n",
      "Multi-Head Attention Output:\n",
      " tensor([[[-1.2236e-01,  1.4848e-01, -1.8296e-01, -3.4761e-01,  7.6765e-03,\n",
      "           2.5413e-01, -2.8464e-01, -4.1895e-01, -1.8783e-01, -5.2990e-02,\n",
      "           1.2894e-02, -1.0938e-01, -1.8527e-01,  1.5226e-01,  7.8006e-02,\n",
      "           1.3754e-01,  1.1739e-01, -3.2933e-01,  8.1063e-02, -2.6789e-02,\n",
      "           3.0602e-01, -1.8980e-01, -1.0885e-01,  2.5736e-01, -2.7368e-02,\n",
      "           7.9552e-02,  6.3158e-03, -1.8686e-01,  3.3936e-01,  2.8623e-01,\n",
      "          -3.8751e-01,  9.9468e-02, -1.7712e-01,  8.0775e-03,  1.0833e-03,\n",
      "           1.3742e-01,  4.6959e-01,  1.4638e-01, -1.4531e-01, -2.3869e-01,\n",
      "          -6.2657e-02, -6.9956e-02,  1.4953e-01,  5.7099e-02, -1.5583e-02,\n",
      "           1.5629e-01, -1.7212e-01,  3.1706e-02,  1.4498e-01, -2.2098e-01,\n",
      "          -5.7287e-02,  1.0331e-01, -4.5108e-02,  8.9436e-02,  1.7245e-02,\n",
      "          -1.2526e-02, -9.9337e-02,  7.7496e-02,  7.9800e-02,  1.0891e-02,\n",
      "           1.0098e-01,  1.6663e-01,  2.9297e-01, -1.5296e-01],\n",
      "         [-1.2262e-01,  1.4712e-01, -1.8353e-01, -3.4857e-01,  7.1824e-03,\n",
      "           2.5532e-01, -2.8353e-01, -4.1995e-01, -1.8721e-01, -5.4903e-02,\n",
      "           1.2395e-02, -1.1132e-01, -1.8551e-01,  1.5117e-01,  7.9318e-02,\n",
      "           1.3792e-01,  1.1646e-01, -3.2881e-01,  8.1538e-02, -2.6419e-02,\n",
      "           3.0751e-01, -1.8847e-01, -1.1069e-01,  2.5737e-01, -2.7265e-02,\n",
      "           7.8469e-02,  4.5609e-03, -1.8684e-01,  3.3766e-01,  2.8558e-01,\n",
      "          -3.8683e-01,  9.8893e-02, -1.7868e-01,  9.6092e-03, -1.5498e-04,\n",
      "           1.3926e-01,  4.6843e-01,  1.4578e-01, -1.4302e-01, -2.3837e-01,\n",
      "          -6.3129e-02, -6.7640e-02,  1.5000e-01,  5.7291e-02, -1.4945e-02,\n",
      "           1.5979e-01, -1.7230e-01,  3.1474e-02,  1.4293e-01, -2.1956e-01,\n",
      "          -5.8166e-02,  1.0093e-01, -4.5030e-02,  8.9801e-02,  1.6107e-02,\n",
      "          -1.1908e-02, -9.7038e-02,  7.8398e-02,  7.6793e-02,  1.1856e-02,\n",
      "           1.0172e-01,  1.6790e-01,  2.9595e-01, -1.5411e-01],\n",
      "         [-1.2166e-01,  1.4696e-01, -1.8382e-01, -3.4978e-01,  1.0060e-02,\n",
      "           2.5666e-01, -2.8508e-01, -4.2050e-01, -1.8648e-01, -5.4333e-02,\n",
      "           1.2902e-02, -1.0957e-01, -1.8549e-01,  1.5162e-01,  7.9227e-02,\n",
      "           1.3814e-01,  1.1919e-01, -3.2930e-01,  8.2726e-02, -2.7508e-02,\n",
      "           3.0964e-01, -1.9090e-01, -1.1173e-01,  2.5915e-01, -2.9733e-02,\n",
      "           7.8601e-02,  8.0325e-03, -1.8645e-01,  3.3950e-01,  2.8632e-01,\n",
      "          -3.8743e-01,  1.0266e-01, -1.8021e-01,  9.9713e-03,  1.3051e-04,\n",
      "           1.3915e-01,  4.6868e-01,  1.4503e-01, -1.4524e-01, -2.4022e-01,\n",
      "          -6.2487e-02, -6.8265e-02,  1.5191e-01,  5.6488e-02, -1.5580e-02,\n",
      "           1.5913e-01, -1.7176e-01,  3.1874e-02,  1.4267e-01, -2.1904e-01,\n",
      "          -5.9184e-02,  1.0220e-01, -4.5161e-02,  9.0340e-02,  1.4916e-02,\n",
      "          -1.3233e-02, -9.9882e-02,  7.9232e-02,  7.7760e-02,  1.2612e-02,\n",
      "           1.0092e-01,  1.6618e-01,  2.9389e-01, -1.5173e-01],\n",
      "         [-1.2264e-01,  1.4788e-01, -1.8371e-01, -3.4932e-01,  7.2958e-03,\n",
      "           2.5583e-01, -2.8432e-01, -4.1855e-01, -1.8778e-01, -5.2086e-02,\n",
      "           1.1845e-02, -1.1250e-01, -1.8582e-01,  1.5349e-01,  7.9878e-02,\n",
      "           1.3829e-01,  1.1719e-01, -3.2965e-01,  8.1694e-02, -2.6030e-02,\n",
      "           3.0762e-01, -1.8912e-01, -1.0885e-01,  2.5805e-01, -2.8059e-02,\n",
      "           7.9389e-02,  6.1521e-03, -1.8854e-01,  3.3774e-01,  2.8580e-01,\n",
      "          -3.8708e-01,  1.0091e-01, -1.7779e-01,  9.1295e-03,  2.4631e-03,\n",
      "           1.3875e-01,  4.6906e-01,  1.4704e-01, -1.4506e-01, -2.3953e-01,\n",
      "          -6.2395e-02, -6.8092e-02,  1.5093e-01,  5.6631e-02, -1.3907e-02,\n",
      "           1.5809e-01, -1.7171e-01,  3.1540e-02,  1.4370e-01, -2.2183e-01,\n",
      "          -5.8016e-02,  1.0328e-01, -4.4370e-02,  8.9198e-02,  1.6405e-02,\n",
      "          -1.2336e-02, -9.7298e-02,  7.8461e-02,  7.8488e-02,  1.3806e-02,\n",
      "           1.0126e-01,  1.6711e-01,  2.9635e-01, -1.5327e-01],\n",
      "         [-1.2106e-01,  1.4644e-01, -1.8527e-01, -3.5112e-01,  9.9029e-03,\n",
      "           2.5580e-01, -2.8385e-01, -4.2209e-01, -1.8438e-01, -5.5004e-02,\n",
      "           1.3891e-02, -1.0904e-01, -1.8326e-01,  1.5053e-01,  7.7635e-02,\n",
      "           1.3845e-01,  1.2022e-01, -3.3047e-01,  8.2405e-02, -2.7586e-02,\n",
      "           3.1066e-01, -1.9057e-01, -1.1349e-01,  2.5964e-01, -2.8434e-02,\n",
      "           7.9648e-02,  8.4985e-03, -1.8555e-01,  3.4060e-01,  2.8524e-01,\n",
      "          -3.8730e-01,  1.0165e-01, -1.8123e-01,  8.2399e-03, -1.0560e-03,\n",
      "           1.3956e-01,  4.6853e-01,  1.4472e-01, -1.4419e-01, -2.4031e-01,\n",
      "          -6.2845e-02, -6.7927e-02,  1.5133e-01,  5.7033e-02, -1.5429e-02,\n",
      "           1.6038e-01, -1.7186e-01,  3.2114e-02,  1.4263e-01, -2.2021e-01,\n",
      "          -5.8594e-02,  1.0280e-01, -4.5315e-02,  9.0506e-02,  1.5416e-02,\n",
      "          -1.3076e-02, -9.8831e-02,  7.9638e-02,  7.6576e-02,  1.2201e-02,\n",
      "           1.0088e-01,  1.6794e-01,  2.9303e-01, -1.5356e-01]]],\n",
      "       device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Attention Weights:\n",
      " torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadAttention with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "seq_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# No mask for now (can add later)\n",
    "mask = None\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention\n",
    "output, attention_weights = multihead_attn(query, key, value, mask)\n",
    "\n",
    "print(\"Multi-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 4, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Multi-Head Attention Output:\n",
      " tensor([[[ 3.1295e-01,  6.5866e-01, -1.0873e-01, -6.9336e-01,  1.0441e-02,\n",
      "          -4.0048e-01,  2.7939e-01, -1.2949e-01],\n",
      "         [ 2.3547e-01,  7.4956e-01,  5.2793e-02, -4.6428e-01, -4.3888e-02,\n",
      "          -3.5918e-01,  2.4268e-01, -2.8908e-01],\n",
      "         [ 5.9905e-01,  7.3684e-01,  1.9799e-01, -4.2775e-01,  6.3794e-03,\n",
      "          -1.1685e-01,  2.3871e-01,  1.8335e-02],\n",
      "         [ 5.5651e-01,  7.0157e-01,  1.1693e-01, -4.2393e-01,  8.3651e-03,\n",
      "          -1.9162e-01,  2.7327e-01, -2.4137e-04]]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n",
      "Attention Weights:\n",
      " tensor([[[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "           [0.5327, 0.4673, 0.0000, 0.0000],\n",
      "           [0.3503, 0.3006, 0.3491, 0.0000],\n",
      "           [0.3382, 0.3141, 0.3477, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "           [0.4870, 0.5130, 0.0000, 0.0000],\n",
      "           [0.3380, 0.3698, 0.2922, 0.0000],\n",
      "           [0.3367, 0.3646, 0.2987, 0.0000]]]]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Padding Mask:\n",
      " tensor([[[[ True,  True,  True, False]]]], device='mps:0')\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], device='mps:0', dtype=torch.uint8)\n",
      "Combined Mask:\n",
      " tensor([[[[1, 0, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0]]]], device='mps:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadAttention with a padding mask and causal mask\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example with 1 padding token\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (bitwise AND to use both padding and causal masks)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0)\n",
    "combined_mask.to(device)\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention with a mask\n",
    "output, attention_weights = multihead_attn(query, key, value, combined_mask)\n",
    "\n",
    "print(\"\\nMulti-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_ff: int, d_model: int, dropout: float = 0.1):\n",
    "        super(PositionwiseFFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.verbose:\n",
    "            print(f\"Input to Encoder Layer: {x.shape}\")\n",
    "        \n",
    "        # Multi-head attention with residual connection and layer normalization\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        if self.verbose:\n",
    "            print(f\"attn_output shape: {attn_output.shape}\")\n",
    "        out1 = self.layernorm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ffn_output))  # Fixed: add out1, not x\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Output from Encoder Layer: {out2.shape}\")\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding mask: tensor([[[[1, 1, 1, 0]]]], device='mps:0', dtype=torch.int32)\n",
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input to Encoder Layer: torch.Size([1, 4, 8])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "attn_output shape: torch.Size([1, 4, 8])\n",
      "Output from Encoder Layer: torch.Size([1, 4, 8])\n",
      "\n",
      "Output from Encoder Layer:\n",
      " tensor([[[ 0.3996, -1.0188,  0.1466,  2.2344, -0.5234, -1.0321,  0.3593,\n",
      "          -0.5657],\n",
      "         [ 0.6747, -1.6915,  0.6398,  1.6503, -0.1632,  0.4911, -0.8221,\n",
      "          -0.7791],\n",
      "         [ 0.8759, -1.2043, -0.5740,  0.8067,  0.2354, -1.7416,  1.2650,\n",
      "           0.3368],\n",
      "         [-0.2022,  0.1511,  1.1835,  0.7018, -0.8649, -1.4601,  1.4789,\n",
      "          -0.9880]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test EncoderLayer with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "d_ff = 16\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random input sequence\n",
    "x = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random padding mask (e.g., if needed)\n",
    "padding_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Example with padding\n",
    "print(f\"Padding mask: {padding_mask.int()}\")\n",
    "\n",
    "# Create EncoderLayer object\n",
    "encoder_layer = EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, verbose=True).to(device)\n",
    "\n",
    "# Pass the input through the encoder layer\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "\n",
    "print(\"\\nOutput from Encoder Layer:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input to Encoder Layer: torch.Size([1, 4, 8])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "attn_output shape: torch.Size([1, 4, 8])\n",
      "Output from Encoder Layer: torch.Size([1, 4, 8])\n",
      "Output from encoder layer with padding mask: tensor([[[-0.1888, -0.0290,  0.1358, -1.5754, -0.4086,  0.8213,  2.0093,\n",
      "          -0.7646],\n",
      "         [ 1.2651,  0.9342, -1.7241, -1.1073, -0.4102,  0.2502,  1.0220,\n",
      "          -0.2296],\n",
      "         [-0.3457,  0.3957, -0.8306, -1.8873,  1.5913,  0.9662,  0.0625,\n",
      "           0.0479],\n",
      "         [ 1.0587, -0.6773, -0.2885, -1.9244, -0.5413,  0.3920,  1.3263,\n",
      "           0.6545]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a padding mask (0 indicates padding)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example sequence with padding\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)\n",
    "\n",
    "# Test EncoderLayer with padding mask\n",
    "encoder_layer = EncoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "x = torch.rand(1, 4, 8).to(device)  # Random input sequence\n",
    "\n",
    "# Pass through the encoder layer with the mask\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "print(\"Output from encoder layer with padding mask:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.src_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape x: {x.shape}\")\n",
    "            print(f\"Encoder output shape: {enc_output.shape}\\n\")\n",
    "        # masked self-attention over the target (with look-ahead mask)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Passing through self-attention\")\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.layernorms[0](x + self.dropout(self_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nPassing Through encoder-decoder attention\")\n",
    "        # encoder-decoder attention over the encoder output (attend to source)\n",
    "        enc_dec_attn_output, _ = self.src_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.layernorms[1](x + self.dropout(enc_dec_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal feedforward of layer\")\n",
    "        # feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorms[2](x + self.dropout(ffn_output))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nOutput shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_length):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(seq_length, seq_length)).type(torch.uint8)  # Shape: (seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input shape x: torch.Size([1, 4, 8])\n",
      "Encoder output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 4, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "Output from decoder layer: tensor([[[ 1.2075, -1.0042,  1.7106, -0.7624, -0.4812,  0.6241, -0.1147,\n",
      "          -1.1797],\n",
      "         [-0.1149, -1.1850,  1.9884,  0.2284, -1.5024, -0.1579,  0.2767,\n",
      "           0.4668],\n",
      "         [ 0.2778, -1.4479,  1.7612,  0.9553, -0.5321,  0.5297, -0.9397,\n",
      "          -0.6042],\n",
      "         [ 0.0765, -0.4631,  2.3884, -0.3595, -0.9824,  0.5013, -0.7469,\n",
      "          -0.4144]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random input sequence for target (decoder input)\n",
    "tgt = torch.rand(1, 4, 8).to(device)  # (batch_size=1, seq_length=4, d_model=8)\n",
    "\n",
    "# Random encoder output (assuming same dimensions for simplicity)\n",
    "enc_output = torch.rand(1, 4, 8).to(device)\n",
    "\n",
    "# Create masks\n",
    "tgt_mask = create_causal_mask(seq_length=4).unsqueeze(0).to(device)  # Causal mask for target\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Padding mask for source\n",
    "\n",
    "# Initialize the decoder layer\n",
    "decoder_layer = DecoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "\n",
    "# Pass through the decoder layer\n",
    "output = decoder_layer(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "print(\"Output from decoder layer:\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # final layer normalization layer\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input of shape: {x.shape}\")\n",
    "        \n",
    "        for i, block in enumerate(self.encoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------ Passing Through Encoder block {i + 1} ----------------\")\n",
    "            \n",
    "            x = block(x, mask=src_mask)\n",
    "\n",
    "        # apply final layer normalization\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal output shape is: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoder.__init__() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m create_padding_mask(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the encoder with 2 blocks for testing\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pass through the encoder\u001b[39;00m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m encoder(src, src_mask\u001b[38;5;241m=\u001b[39msrc_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.__init__() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "# Random input sequence (batch_size=1, seq_length=4, d_model=8)\n",
    "src = torch.rand(1, 4, 8)\n",
    "\n",
    "# Create a padding mask for the source sequence\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]]))\n",
    "\n",
    "# Initialize the encoder with 2 blocks for testing\n",
    "encoder = Encoder(num_blocks=2, num_heads=2, d_model=8, d_ff=16, dropout=0.1, max_length=30, verbose=True)\n",
    "\n",
    "# Pass through the encoder\n",
    "output = encoder(src, src_mask=src_mask)\n",
    "print(\"Final output from encoder:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_mask', 'tgt_mask'])\n",
      "Source tokens: torch.Size([4, 30])\n",
      "tgt_input: torch.Size([4, 29])\n",
      "tgt_output: torch.Size([4, 29])\n",
      "src_mask: torch.Size([4, 1, 30])\n",
      "tgt_mask: torch.Size([4, 29, 29])\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "\n",
      "Source token shape: torch.Size([4, 30])\n",
      "Input of shape: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 1 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 2 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 3 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 4 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 5 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 6 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "Final output shape is: torch.Size([4, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "# test with actual examples\n",
    "batch_size = 4\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(batch.keys())\n",
    "    src_tokens = batch['src_tokens'].to(device)  # The tokenized source sentences\n",
    "    tgt_input = batch['tgt_input'].to(device)  # The tokenized target sentences\n",
    "    tgt_output = batch['tgt_output'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "    tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "    print(f\"Source tokens: {src_tokens.shape}\")\n",
    "    print(f\"tgt_input: {tgt_input.shape}\")\n",
    "    print(f\"tgt_output: {tgt_output.shape}\")\n",
    "    print(f\"src_mask: {src_mask.shape}\")\n",
    "    print(f\"tgt_mask: {tgt_mask.shape}\")\n",
    "\n",
    "    break  # Just getting the first batch for demonstration\n",
    "\n",
    "encoder = Encoder(num_blocks=6, num_heads=8, d_model=512, d_ff=2048, verbose=True).to(device)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), 512).to(device)\n",
    "pos_encoder = PositionalEncoding(512, dropout=0.1, max_len=512).to(device)\n",
    "\n",
    "for batch in small_dl:\n",
    "    src_tokens = batch['src_tokens'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "\n",
    "    print(f\"\\nSource token shape: {src_tokens.shape}\")\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose)\n",
    "        ] for _ in range(num_blocks))\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------- Passing Through Decoder Block {i+1} ----------------\")\n",
    "            tgt = block(tgt, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.layernorm(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "\n",
      "------------- Passing Through Decoder Block 1 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "Decoder output shape: torch.Size([4, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Mock data for testing\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "d_ff = 2048\n",
    "\n",
    "# Random embedded target tokens (already embedded, just mock data)\n",
    "tgt_embed = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Random encoder output (to simulate the output from the encoder)\n",
    "enc_output = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Create padding mask (mock data, assume no padding tokens for simplicity)\n",
    "src_mask = torch.ones(batch_size, 1, seq_length)  # Shape: (batch_size, 1, seq_length)\n",
    "print(src_mask.shape)\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones(batch_size, seq_length, seq_length))\n",
    "print(tgt_mask.shape)\n",
    "\n",
    "\n",
    "# Initialize the decoder without embedding\n",
    "decoder = Decoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=0.1, verbose=True)\n",
    "\n",
    "# Pass the mock data through the decoder\n",
    "decoder_output = decoder(tgt_embed, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to instantiate an encoder and decoder class and string them together to confirm that everything works together. Than we will abstract and create an Encoder-Decoder sequence to sequence model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define the linear + softmax step for generating token probabilities.\n",
    "        Layer projects vector on to vocab space and then applys a log_softmax. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Source tokens: torch.Size([4, 30])\n",
      "Target input tokens: torch.Size([4, 29])\n",
      "Target output tokens: torch.Size([4, 29])\n",
      "Source mask: torch.Size([4, 1, 30])\n",
      "Target mask: torch.Size([4, 29, 29])\n",
      "Input of shape: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 1 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 2 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 3 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 4 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 5 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 6 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "Final output shape is: torch.Size([4, 30, 512])\n",
      "Encoder output: torch.Size([4, 30, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 1 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "tensor([[[-10.1509, -11.7729, -10.4918,  ..., -11.2053, -10.9931, -10.2339],\n",
      "         [-10.2825, -10.5184, -10.8715,  ..., -10.3931, -10.6028,  -9.9379],\n",
      "         [ -9.9817, -11.1991, -10.0541,  ..., -11.2393, -10.6914, -11.2956],\n",
      "         ...,\n",
      "         [-10.7318, -11.0253, -12.0622,  ..., -11.1931, -10.7965, -10.0273],\n",
      "         [-10.4851, -10.8293, -11.9257,  ..., -11.2111, -10.5220, -10.4527],\n",
      "         [-10.5266, -10.8634, -11.5850,  ..., -11.4621, -10.7497, -10.1334]],\n",
      "\n",
      "        [[ -9.7751, -11.5536, -10.6933,  ..., -10.8980, -11.3418, -10.4738],\n",
      "         [-10.3557, -11.6661, -10.4140,  ..., -11.2706, -10.6414, -11.4845],\n",
      "         [ -9.6073, -10.4230, -11.1468,  ..., -11.4061, -10.6558, -10.3891],\n",
      "         ...,\n",
      "         [-10.4400, -11.0612, -11.2259,  ..., -11.5149, -10.3519,  -9.9005],\n",
      "         [ -9.9013, -11.2558, -10.8587,  ..., -11.0375, -10.7416,  -9.9862],\n",
      "         [-10.5282, -10.7430, -11.9625,  ..., -10.8450, -10.3283, -10.1472]],\n",
      "\n",
      "        [[ -9.9455, -12.0736, -10.9630,  ..., -11.2621, -10.8488, -10.6140],\n",
      "         [-10.5105, -11.4781,  -9.9865,  ..., -10.6689, -10.8186, -10.6073],\n",
      "         [ -9.3751, -11.7043, -10.8552,  ..., -10.8974, -10.5407, -11.0471],\n",
      "         ...,\n",
      "         [-10.7683, -10.7618, -11.8876,  ..., -11.4014, -10.1262,  -9.5814],\n",
      "         [-11.0229, -11.8734, -11.7436,  ..., -11.6711, -11.0447,  -9.6907],\n",
      "         [-10.1751, -10.9023, -10.8887,  ..., -11.1676, -11.3249, -10.1306]],\n",
      "\n",
      "        [[ -9.5224, -11.5780, -10.8259,  ..., -10.9938, -10.8554, -10.3780],\n",
      "         [-11.0853, -11.1724, -10.5813,  ..., -10.5131,  -9.7080, -10.7322],\n",
      "         [-10.0055, -10.7317, -11.0437,  ..., -11.5776, -10.5506, -10.5374],\n",
      "         ...,\n",
      "         [-10.3010, -12.0910, -10.0151,  ..., -12.9259, -10.5342, -10.3923],\n",
      "         [-10.1451, -10.3923, -10.4828,  ..., -10.8584, -11.0348, -10.5937],\n",
      "         [-10.5766, -10.5244, -10.5483,  ..., -11.6782,  -9.5504, -10.5284]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[  897,  7974,   393,   484,   862,   324,  3115,  7396,   280,  4061,\n",
      "           304, 14764,  6352,    18,     2,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 1589,   541,   324,  5136,   569,   280,  3193,   312,  9376,   393,\n",
      "           766,   520,   812,   264,  1679,   304, 16621, 17198,   555,   312,\n",
      "         18932,   918,   286, 20043, 24555,    18,     2,     1,     1],\n",
      "        [   45,  1991,   393,   280,  4061,   304, 14764,  6352,   326,   264,\n",
      "          4234,  2746,  4061,   304,   280,  2299,  8712,  1458,   312,   264,\n",
      "          2217,   313,  1104,  4496,   435,   795,   324, 14721,   286],\n",
      "        [17077,   435,   324,  4790,   372,   538,    16,  6029,   992,    16,\n",
      "           313, 10632,   264, 10732,   313,   280, 20043,   398,   747,   279,\n",
      "           992, 20944,  1140,   724, 10837,   527,  1201,   312,   280]])\n",
      "torch.Size([4, 29, 37000])\n",
      "torch.Size([4, 29])\n",
      "torch.Size([4, 29])\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 30\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len)\n",
    "\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target input tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "\n",
    "    src_tokens = batch['src_tokens']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_input = batch['tgt_input']\n",
    "    tgt_output = batch['tgt_output']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_mask = batch['tgt_mask']\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "\n",
    "    print(f\"Encoder output: {encoder_output.shape}\")\n",
    "\n",
    "    tgt_embed = embedding(tgt_input)\n",
    "    tgt_embed = pos_encoder(tgt_embed)\n",
    "    dec_output = decoder(tgt=tgt_embed, enc_output = encoder_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "    output = generator(dec_output)\n",
    "    predicted_tokens = torch.argmax(output, dim=-1)\n",
    "\n",
    "    print(output)\n",
    "    print(tgt_output)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(predicted_tokens.shape)\n",
    "    print(tgt_output.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we abstract the above into a EncoderDecoder class. \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, generator: Generator, embedding: nn.Embedding, pos_encoder: PositionalEncoding, verbose: bool = False):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "        self.embedding = embedding\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, src_tokens, tgt_input, src_mask, tgt_mask):\n",
    "        # Encoder\n",
    "        src_embed = self.embedding(src_tokens)\n",
    "        src_embed = self.pos_encoder(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_embed = self.embedding(tgt_input)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        dec_output = self.decoder(tgt=tgt_embed, enc_output=encoder_output, src_mask=src_mask, tgt_mask = tgt_mask)\n",
    "\n",
    "        output_log_probs = self.generator(dec_output)\n",
    "\n",
    "        return output_log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_ds = TranslationDataset(dataset['train'].shuffle().select(range(20000)), tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)\n",
    "val_ds = TranslationDataset(dataset['validation'], tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_mask', 'tgt_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate models\n",
    "\n",
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 512\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model).to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len).to(device)\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder, generator, embedding, pos_encoder, verbose=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate scheduler, following `Attention is All You Need` for now. \n",
    "# lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n",
    "warmup_steps = 4000\n",
    "\n",
    "def get_lr(step_num):\n",
    "    return d_model ** -0.5 * min(step_num ** -0.5, step_num * warmup_steps ** -1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378d3b304af74d3285f95538d3e4d1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss:  0.8573\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ffaf3c6960461fbdafe0de629a3663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average validation loss: 0.3395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b47eb0d58d4491bb1d1179f229fdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss:  0.3683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d911b8f253bc4530bf3937f11dc7efa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average validation loss: 0.3250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4fdfeb43ca415b9b4acd1b54da9dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss:  0.3486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1882fa68fe24d738ec58e1687e53b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average validation loss: 0.3167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8535821ea8bd4036ad358f3e49b8c1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss:  0.3330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b7b66237184c148d3d40005b6f55cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average validation loss: 0.3088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4501ef15e37249db8c9051ef8bb21f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss:  0.3201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b0963ad314ed4b31048a39db75725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average validation loss: 0.3057\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# training loop\n",
    "# optimizer and criterion\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step_num = 0\n",
    "\n",
    "    for batch in tqdm(train_dl):\n",
    "        step_num += 1\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = get_lr(step_num)\n",
    "\n",
    "        src_tokens = batch['src_tokens'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        src_mask = batch['src_mask'].to(device)\n",
    "        tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "        # print(src_tokens.device)\n",
    "        # print(src_mask.device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_logits = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = criterion(output_logits.view(-1, output_logits.size(-1)), tgt_output.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {avg_loss: .4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl):\n",
    "            src_tokens = batch['src_tokens'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_output = batch['tgt_output'].to(device)\n",
    "            src_mask = batch['src_mask'].to(device)\n",
    "            tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "            output = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dl)\n",
    "    print(F\"Epoch {epoch + 1}/{num_epochs}, Average validation loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src_sentence': 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten',\n",
       "  'tgt_sentence': 'A Republican strategy to counter the re-election of Obama',\n",
       "  'src_tokens': tensor([    0,  2530,  2878, 12244,  8708,  4789,    16,   577,   319,  4755,\n",
       "           3815,   408, 11741,  7738, 27237,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,    37,  5929,   279,  4024,   313,  7273,   280,   351,    17,\n",
       "           4357,   675,   304, 11741,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.',\n",
       "  'tgt_sentence': 'Republican leaders justified their policy by the need to combat electoral fraud.',\n",
       "  'src_tokens': tensor([    0,   567, 29354,  6387,   319, 30529, 12897, 20929,  1090,  2346,\n",
       "            437,   319,  5622,    16,   389,  3041,  3502, 27218,   386, 12398,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0, 25904,  1312,   279,  5465, 14716,   808,  1458,   519,   280,\n",
       "            943,   313,  8810, 18700, 10601,    18,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.',\n",
       "  'tgt_sentence': 'However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.',\n",
       "  'src_tokens': tensor([    0,  8067,  9096,   384, 13181,   279,  7277, 13557,   268,   430,\n",
       "            719, 25411,   394,    16,  3623,   531, 21506,    16,   521,   319,\n",
       "           3041,  3502, 27218,   286,   389,  2821, 10610,   262,   423,   475,\n",
       "            317,  5697,   319,  1194, 35674,  4968,  1423,   365,  9984,  1228,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  2395,    16,   280, 13181,   279,  6785, 16453,   484,   264,\n",
       "          19725,    16, 24799,   393, 18700, 10601,   326,   409,   285,   262,\n",
       "            286,   280,  2713,  1162,  1189,   280,  1679,   304,  1120, 14608,\n",
       "            519,  3772,  4097,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Die Rechtsanwälte der Republikaner haben in 10 Jahren in den USA übrigens nur 300 Fälle von Wahlbetrug verzeichnet.',\n",
       "  'tgt_sentence': 'Indeed, Republican lawyers identified only 300 cases of electoral fraud in the United States in a decade.',\n",
       "  'src_tokens': tensor([    0,   567, 31236, 19176,   319, 30529, 12897,   672,   286,  1523,\n",
       "           1802,   286,   389,  2821, 13316,   848,  6139, 11221,   408,  3041,\n",
       "           3502, 27218, 34763,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  8342,    16,  5929,   279, 23670, 13771,   962,  6139,  4545,\n",
       "            304, 18700, 10601,   286,   280,  2713,  1162,   286,   264, 15815,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Eins ist sicher: diese neuen Bestimmungen werden sich negativ auf die Wahlbeteiligung auswirken.',\n",
       "  'tgt_sentence': 'One thing is certain: these new provisions will have a negative impact on voter turn-out.',\n",
       "  'src_tokens': tensor([    0,  1568,    87,   423,  2276,    30,   843,  1777,  6256,   514,\n",
       "            508, 20577,   428,   317,  3041, 29507, 18584,    18,     2,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([   0, 4342, 4839,  326, 2072,   30, 1032,  874, 6454,  541,  520,  264,\n",
       "          7650, 4926,  385, 2493,  338, 3173,   17,  688,   18,    2,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1])},\n",
       " {'src_sentence': 'In diesem Sinne untergraben diese Maßnahmen teilweise das demokratische System der USA.',\n",
       "  'tgt_sentence': 'In this sense, the measures will partially undermine the American democratic system.',\n",
       "  'src_tokens': tensor([    0,   588,  1060,  6632, 23110,   843,  2092,  8778,   384, 11022,\n",
       "           2705,   319,  2821,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   588,   484,  5477,    16,   280,  2372,   541, 23351, 20676,\n",
       "            280,  4984,  4696,  1419,    18,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Im Gegensatz zu Kanada sind die US-Bundesstaaten für die Durchführung der Wahlen in den einzelnen Staaten verantwortlich.',\n",
       "  'tgt_sentence': 'Unlike in Canada, the American States are responsible for the organisation of federal elections in the United States.',\n",
       "  'src_tokens': tensor([    0,  1501, 10183,   386, 15817,   580,   317,  1377,    17, 33664,\n",
       "           1296,   430,   317,  8023,   319,  8005,   286,   389,  4851,  3122,\n",
       "           7388,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0, 25618,   286, 12688,    16,   280,  4984,  1162,   439,  4276,\n",
       "            372,   280,  8848,   304, 14593,  6752,   286,   280,  2713,  1162,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'In diesem Sinne hat die Mehrheit der amerikanischen Regierungen seit 2009 neue Gesetze verkündet, die das Verfahren für die Registrierung oder den Urnengang erschweren.',\n",
       "  'tgt_sentence': 'It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.',\n",
       "  'src_tokens': tensor([    0,   588,  1060,  6632,   647,   317,  5866,   319,  9309,  5915,\n",
       "           2136,  2245,  1663, 11424, 36981,    16,   317,   384,  4528,   430,\n",
       "            317, 15918,   653,   389,  2435,   472,  2487, 33852,    18,     2,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   897,   326,   286,   484,  6897,   393,   264,  4871,   304,\n",
       "           4984,  5601,   520,  9975,   874,  7871,  2255,  2245,  3274,   280,\n",
       "          10713,   494,  7287,  1539,   801,  2855,    18,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Dieses Phänomen hat nach den Wahlen vom November 2010 an Bedeutung gewonnen, bei denen 675 neue republikanische Vertreter in 26 Staaten verzeichnet werden konnten.',\n",
       "  'tgt_sentence': 'This phenomenon gained momentum following the November 2010 elections, which saw 675 new Republican representatives added in 26 States.',\n",
       "  'src_tokens': tensor([    0,  3890, 19491,   647,   715,   389,  8005,  1194,  4334,  2789,\n",
       "            293,  2819, 12400,    16,   690,  1856,  1322,  9528,  1663,  2878,\n",
       "          12244,  8708,  7001,   286,  6225,  3122, 34763,   514,  6518,    18,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   887, 16175, 14218, 28840,  3301,   280,  4334,  2789,  6752,\n",
       "             16,   557, 10104,  1322,  9528,   874,  5929,   279,  7598,  5781,\n",
       "            286,  6225,  1162,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Infolgedessen wurden 180 Gesetzesentwürfe allein im Jahr 2011 eingeführt, die die Ausübung des Wahlrechts in 41 Staaten einschränken.',\n",
       "  'tgt_sentence': 'As a result, 180 bills restricting the exercise of the right to vote in 41 States were introduced in 2011 alone.',\n",
       "  'src_tokens': tensor([    0,   588, 28261,  1527, 18189,  4219,  1206, 27824,  6223,   414,\n",
       "            904,  9274,  9657,    16,   317,   317, 19744,   434,  3041,  4739,\n",
       "            286, 18439,  3122, 30130,    18,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  1683,   264,  2404,    16, 18189, 36515, 35318,   280,  9761,\n",
       "            304,   280,  1475,   313,  2675,   286, 18439,  1162,  1121,  8094,\n",
       "            286,  9274,  7757,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])}]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if inference works\n",
    "num_examples = 10\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(val_ds[i])\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model: EncoderDecoder, src_tokens: torch.Tensor, tokenizer: ByteLevelBPETokenizer = tokenizer, pad_token_id: int = PAD_TOKEN_ID, max_len=100):\n",
    "    model.eval()\n",
    "\n",
    "    # embed the source tokens and create the src_mask\n",
    "    src_tokens = src_tokens.unsqueeze(0).to(device)\n",
    "    src_mask = (src_tokens != pad_token_id).unsqueeze(0).to(device) # shape: (1, seq_length)\n",
    "\n",
    "    src_embed = model.embedding(src_tokens)\n",
    "    src_embed = model.pos_encoder(src_embed)\n",
    "\n",
    "    # store encoder hidden states for the src_tokens\n",
    "    encoder_output = model.encoder(src_embed, src_mask)\n",
    "\n",
    "    # initizlie target sentence with BOS token\n",
    "    tgt_tokens = torch.tensor([BOS_TOKEN_ID], dtype=torch.long).to(device)\n",
    "\n",
    "    # Autoregressive loop to generate sentence\n",
    "    for _ in range(max_len):\n",
    "        # create target mask\n",
    "        tgt_seq_len = tgt_tokens.size(0)\n",
    "        tgt_mask = torch.tril(torch.ones(1, tgt_seq_len, tgt_seq_len)).to(device)\n",
    "        #print(f\"target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "        #print(f\"Tokens at beginning: {tgt_tokens}\")\n",
    "        tgt_embed = model.embedding(tgt_tokens).unsqueeze(0)\n",
    "        #print(f\"Token embeddings shape: {tgt_embed.shape}\")\n",
    "        tgt_embed = model.pos_encoder(tgt_embed)\n",
    "\n",
    "        output_logits = model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "        output_log_probs = model.generator(output_logits)\n",
    "        # print(output_log_probs.shape)\n",
    "        # print(output_log_probs)\n",
    "        next_token = torch.argmax(output_log_probs[:, -1, :], dim=-1)\n",
    "        # print(next_token.shape)\n",
    "        #print(f\"Next token: {next_token.item()}\")\n",
    "        #print(tgt_tokens.shape)\n",
    "        # append next\n",
    "        tgt_tokens = torch.cat([tgt_tokens, next_token])\n",
    "\n",
    "        if next_token.item() == EOS_TOKEN_ID or tgt_tokens.size(0) >= 50:\n",
    "            break\n",
    "\n",
    "        #print(f\"Resulting Target Tokens: {tgt_tokens}\")\n",
    "\n",
    "    print(f\"Source sentence: {tokenizer.decode([num for num in src_tokens.squeeze(0).tolist() if num != pad_token_id], skip_special_tokens=True)}\")\n",
    "    print(f\"Translation: {tokenizer.decode(tgt_tokens.tolist())}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=10):\n",
    "    values, indices = torch.topk(logits, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Mehrere Gesetzesentwürfe wurden durch die Vetos der demokratischen Gouverneure blockiert.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(src_tokens.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(BOS_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: <s>Er wird von Co-Trainern wie Claude Makelele unterstützt, der auf derselben Position spielte wie ich.</s>\n",
      "Translation: <s>The first of the most of the world's years, as I am as the most of the world.</s>\n",
      "Source sentence: <s>Sie scheinen das Risiko für Brust-, Grimmdarm-, Mastdarm-, Speiseröhren-, Bauchspeicheldrüsen- und Gebärmutterkrebs zu erhöhen.</s>\n",
      "Translation: <s>You can be the same time for the first time, the first time to the world of the world's leading to the world.</s>\n",
      "Source sentence: <s>Die Überlebenschancen von Hélène Richard waren bereits minimal, als sie eine beschwerliche Chemotherapie abgebrochen hat.</s>\n",
      "Translation: <s>The first of the world's years has been made a large number of the world, which has been made by the world.</s>\n",
      "Source sentence: <s>Der PSA-Test weise in der Tat manchmal fehlerhafte Ergebnisse auf, mit falschen negativen oder aber auch falschen positiven Ergebnissen, die zu unnötigen medizinischen Eingriffen führen.</s>\n",
      "Translation: <s>The hotel is the best possible of the world, but also with the world, but also the best way to the other hand, but also the best of the world.</s>\n",
      "Source sentence: <s>Dieses Jahr liegt es uns wirklich am Herzen, diesen Titel abzusahnen.</s>\n",
      "Translation: <s>This is the only one of the hotel, we are the right of this year.</s>\n",
      "Source sentence: <s>Stabileres Feld</s>\n",
      "Translation: <s>The guest reviews are submitted by our customers after their stay at Hotel</s>\n",
      "Source sentence: <s>Man schlägt ihnen eine aktive Überwachung vor und bietet ihnen bei Fortschreiten der Krankheit eine Behandlung an.</s>\n",
      "Translation: <s>You can be a good and the best of the world.</s>\n",
      "Source sentence: <s>Wir haben mit 79 Punkten abgeschlossen.</s>\n",
      "Translation: <s>We have been made with the world.</s>\n",
      "Source sentence: <s>Sie sagt, dass das Verfahren \"schmerzhaft, aber positiv\" sei.</s>\n",
      "Translation: <s>You can be the fact that the first time, but it is not only one of the same.</s>\n",
      "Source sentence: <s>Aber durch die Krankheit habe ich meine Kinder wirklich kennen gelernt.</s>\n",
      "Translation: <s>But I have been said that I have been made.</s>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_integers = np.random.choice(300, 10, replace=False).tolist()\n",
    "examples = [val_ds[i]['src_tokens'] for i in random_integers]\n",
    "for example in examples:\n",
    "    greedy_decoding(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = val_ds[237]['src_tokens'].unsqueeze(0).to(device)\n",
    "src_sentence = val_ds[237]['src_sentence']\n",
    "src_mask = (src_tokens != PAD_TOKEN_ID).unsqueeze(0).to(device)\n",
    "src_mask.shape, src_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[213, 227, 237, 272, 175, 31, 15, 183, 243, 159]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embed = model.embedding(src_tokens)\n",
    "src_embed = model.pos_encoder(src_embed)\n",
    "encoder_output = model.encoder(src_embed, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target mask shape: torch.Size([1, 1, 1])\n",
      "Tokens at beginning: tensor([0], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 1, 512])\n",
      "None\n",
      "Next token: 465\n",
      "torch.Size([1])\n",
      "Resulting Target Tokens: tensor([  0, 465], device='mps:0')\n",
      "target mask shape: torch.Size([1, 2, 2])\n",
      "Tokens at beginning: tensor([  0, 465], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 2, 512])\n",
      "None\n",
      "Next token: 791\n",
      "torch.Size([2])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791], device='mps:0')\n",
      "target mask shape: torch.Size([1, 3, 3])\n",
      "Tokens at beginning: tensor([  0, 465, 791], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 3, 512])\n",
      "None\n",
      "Next token: 326\n",
      "torch.Size([3])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326], device='mps:0')\n",
      "target mask shape: torch.Size([1, 4, 4])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 4, 512])\n",
      "None\n",
      "Next token: 264\n",
      "torch.Size([4])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264], device='mps:0')\n",
      "target mask shape: torch.Size([1, 5, 5])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 5, 512])\n",
      "None\n",
      "Next token: 874\n",
      "torch.Size([5])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874], device='mps:0')\n",
      "target mask shape: torch.Size([1, 6, 6])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 6, 512])\n",
      "None\n",
      "Next token: 313\n",
      "torch.Size([6])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313], device='mps:0')\n",
      "target mask shape: torch.Size([1, 7, 7])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 7, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([7])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280], device='mps:0')\n",
      "target mask shape: torch.Size([1, 8, 8])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 8, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([8])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 9, 9])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 9, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([9])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 10, 10])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 10, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([10])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16], device='mps:0')\n",
      "target mask shape: torch.Size([1, 11, 11])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 11, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([11])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 12, 12])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 12, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([12])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 13, 13])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 13, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([13])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 14, 14])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 14, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([14])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 15, 15])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 15, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([15])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16], device='mps:0')\n",
      "target mask shape: torch.Size([1, 16, 16])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 16, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([16])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312], device='mps:0')\n",
      "target mask shape: torch.Size([1, 17, 17])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 17, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([17])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280], device='mps:0')\n",
      "target mask shape: torch.Size([1, 18, 18])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 18, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([18])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 19, 19])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 19, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([19])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 20, 20])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 20, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([20])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16], device='mps:0')\n",
      "target mask shape: torch.Size([1, 21, 21])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 21, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([21])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312], device='mps:0')\n",
      "target mask shape: torch.Size([1, 22, 22])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 22, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([22])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280], device='mps:0')\n",
      "target mask shape: torch.Size([1, 23, 23])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 23, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([23])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 24, 24])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 24, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([24])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 25, 25])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 25, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([25])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 26, 26])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 26, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([26])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 27, 27])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 27, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([27])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 28, 28])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 28, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([28])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 29, 29])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 29, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([29])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 30, 30])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 30, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([30])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16], device='mps:0')\n",
      "target mask shape: torch.Size([1, 31, 31])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 31, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([31])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312], device='mps:0')\n",
      "target mask shape: torch.Size([1, 32, 32])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 32, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([32])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280], device='mps:0')\n",
      "target mask shape: torch.Size([1, 33, 33])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 33, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([33])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 34, 34])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 34, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([34])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "target mask shape: torch.Size([1, 35, 35])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 35, 512])\n",
      "None\n",
      "Next token: 16\n",
      "torch.Size([35])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16], device='mps:0')\n",
      "target mask shape: torch.Size([1, 36, 36])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 36, 512])\n",
      "None\n",
      "Next token: 312\n",
      "torch.Size([36])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312], device='mps:0')\n",
      "target mask shape: torch.Size([1, 37, 37])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 37, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([37])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280], device='mps:0')\n",
      "target mask shape: torch.Size([1, 38, 38])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 38, 512])\n",
      "None\n",
      "Next token: 670\n",
      "torch.Size([38])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "target mask shape: torch.Size([1, 39, 39])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 39, 512])\n",
      "None\n",
      "Next token: 734\n",
      "torch.Size([39])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 40, 40])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 40, 512])\n",
      "None\n",
      "Next token: 18\n",
      "torch.Size([40])\n",
      "Resulting Target Tokens: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  18],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 41, 41])\n",
      "Tokens at beginning: tensor([  0, 465, 791, 326, 264, 874, 313, 280, 670, 734,  16, 312, 280, 670,\n",
      "        734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  16, 312, 280,\n",
      "        670, 734,  16, 312, 280, 670, 734,  16, 312, 280, 670, 734,  18],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 41, 512])\n",
      "None\n",
      "Next token: 2\n",
      "torch.Size([41])\n"
     ]
    }
   ],
   "source": [
    "tgt_tokens = torch.tensor([BOS_TOKEN_ID], dtype=torch.long).to(device)\n",
    "# tgt_mask = torch.ones(1, 1).to(device)\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    # create target mask\n",
    "    tgt_seq_len = tgt_tokens.size(0)\n",
    "    tgt_mask = torch.tril(torch.ones(1, tgt_seq_len, tgt_seq_len)).to(device)\n",
    "    print(f\"target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "    print(f\"Tokens at beginning: {tgt_tokens}\")\n",
    "    tgt_embed = model.embedding(tgt_tokens).unsqueeze(0)\n",
    "    print(print(f\"Token embeddings shape: {tgt_embed.shape}\"))\n",
    "    tgt_embed = model.pos_encoder(tgt_embed)\n",
    "\n",
    "    output_logits = model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "    output_log_probs = model.generator(output_logits)\n",
    "    # print(output_log_probs.shape)\n",
    "    # print(output_log_probs)\n",
    "    next_token = torch.argmax(output_log_probs[:, -1, :], dim=-1)\n",
    "    # print(next_token.shape)\n",
    "    print(f\"Next token: {next_token.item()}\")\n",
    "    print(tgt_tokens.shape)\n",
    "    # append next\n",
    "    tgt_tokens = torch.cat([tgt_tokens, next_token])\n",
    "\n",
    "    if next_token.item() == EOS_TOKEN_ID or tgt_tokens.size(0) >= 50:\n",
    "        break\n",
    "\n",
    "    print(f\"Resulting Target Tokens: {tgt_tokens}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 465,\n",
       " 791,\n",
       " 326,\n",
       " 264,\n",
       " 874,\n",
       " 313,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 16,\n",
       " 312,\n",
       " 280,\n",
       " 670,\n",
       " 734,\n",
       " 18,\n",
       " 2]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als Bürgermeisterin des Stadtbezirks Rivière-des-Prairies, im Osten der Insel, lehnte sie sich ab 2010 gegen den Verkauf eines Gemeindegrundstücks auf, dass für 5 Millionen Dollar erworben und für 1,6 Millionen an Bauträger im boomenden Immobilienmarkt weiterverkauft wurde.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>The Commission is a new to the European Union, and the European Union, and the European Union, and the European Union, and the European Union, and the European Union, and the European Union.</s>'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(src_sentence)\n",
    "tokenizer.decode(tgt_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between example 0 and example 1: 434.9283447265625\n",
      "Difference between example 0 and example 2: 425.0646667480469\n",
      "Difference between example 0 and example 3: 437.7772216796875\n",
      "Difference between example 0 and example 4: 418.0791931152344\n",
      "Difference between example 0 and example 5: 413.323486328125\n",
      "Difference between example 0 and example 6: 458.239501953125\n",
      "Difference between example 0 and example 7: 441.1026306152344\n",
      "Difference between example 0 and example 8: 427.374755859375\n",
      "Difference between example 0 and example 9: 431.8225402832031\n",
      "Difference between example 1 and example 2: 367.7225646972656\n",
      "Difference between example 1 and example 3: 349.079345703125\n",
      "Difference between example 1 and example 4: 398.0154113769531\n",
      "Difference between example 1 and example 5: 380.4783935546875\n",
      "Difference between example 1 and example 6: 358.9228820800781\n",
      "Difference between example 1 and example 7: 394.5393981933594\n",
      "Difference between example 1 and example 8: 380.1678466796875\n",
      "Difference between example 1 and example 9: 341.3323974609375\n",
      "Difference between example 2 and example 3: 362.7933349609375\n",
      "Difference between example 2 and example 4: 376.490966796875\n",
      "Difference between example 2 and example 5: 392.9364318847656\n",
      "Difference between example 2 and example 6: 384.8287353515625\n",
      "Difference between example 2 and example 7: 385.8368835449219\n",
      "Difference between example 2 and example 8: 355.4947814941406\n",
      "Difference between example 2 and example 9: 347.1436767578125\n",
      "Difference between example 3 and example 4: 394.44244384765625\n",
      "Difference between example 3 and example 5: 406.8652648925781\n",
      "Difference between example 3 and example 6: 393.43408203125\n",
      "Difference between example 3 and example 7: 428.79205322265625\n",
      "Difference between example 3 and example 8: 364.1131591796875\n",
      "Difference between example 3 and example 9: 341.3028259277344\n",
      "Difference between example 4 and example 5: 375.1700439453125\n",
      "Difference between example 4 and example 6: 402.3502197265625\n",
      "Difference between example 4 and example 7: 393.33782958984375\n",
      "Difference between example 4 and example 8: 368.0186462402344\n",
      "Difference between example 4 and example 9: 391.9217224121094\n",
      "Difference between example 5 and example 6: 364.4927673339844\n",
      "Difference between example 5 and example 7: 364.3221130371094\n",
      "Difference between example 5 and example 8: 381.7942810058594\n",
      "Difference between example 5 and example 9: 381.9730224609375\n",
      "Difference between example 6 and example 7: 352.6765441894531\n",
      "Difference between example 6 and example 8: 366.41229248046875\n",
      "Difference between example 6 and example 9: 351.6480712890625\n",
      "Difference between example 7 and example 8: 368.2718505859375\n",
      "Difference between example 7 and example 9: 374.1487731933594\n",
      "Difference between example 8 and example 9: 344.01055908203125\n"
     ]
    }
   ],
   "source": [
    "# collect encodings for the src sentences\n",
    "encodings = []\n",
    "for example in examples:\n",
    "    src_tokens = example['src_tokens'].unsqueeze(0).to(device)\n",
    "    src_mask = (src_tokens != PAD_TOKEN_ID).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_embed = model.embedding(src_tokens)\n",
    "        src_embed = model.pos_encoder(src_embed)\n",
    "        encoder_output = model.encoder(src_embed, src_mask)\n",
    "\n",
    "    encodings.append(encoder_output)\n",
    "\n",
    "encoder_diffs = []\n",
    "for i in range(9):\n",
    "    for j in range(i+1, 10):\n",
    "        diff = torch.norm(encodings[i] - encodings[j])\n",
    "        encoder_diffs.append(diff)\n",
    "        print(f\"Difference between example {i} and example {j}: {diff}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(116.9367, device='mps:0')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(encoder_diffs) - min(encoder_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_sentence': 'Meine Damen, dies bedeutet das Ende des kleinen finanziellen Vorteils, den sie genießen, weil sie vorsichtiger fahren und länger leben.',\n",
       " 'tgt_sentence': 'Ladies, this marks the end of the small financial advantage you enjoy for being more careful drivers and for living longer.',\n",
       " 'src_tokens': tensor([    0,  7446,  4611,    16,   452,  4134,   384,  3134,   434,  4458,\n",
       "          8411, 10582,    87,    16,   389,   673,  6403,    16,  1982,   673,\n",
       "           595,  1224,  1101,  8532,   320,  7781,  5517,    18,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " 'tgt_tokens': tensor([    0, 14287,    16,   484, 16711,   280,  1407,   304,   280,  1966,\n",
       "          2161,  7015,   538,  2940,   372,  1482,   801, 13049, 13096,   312,\n",
       "           372,  4702,  4248,    18,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1])}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
