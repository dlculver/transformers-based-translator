{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WMT14 dataset for German-English translation\n",
    "dataset = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a very small segment for experimentation\n",
    "# Take a small subset for experimentation\n",
    "small_train_dataset = dataset['train'].select(range(20))\n",
    "small_val_dataset = dataset['validation'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are following the original `Attention is all you need paper` we will use Byte-Pair Encoding\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"bpe_tokenizer/vocab.json\",\n",
    "    \"bpe_tokenizer/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Das ist ein Beispiel.\").ids)\n",
    "\n",
    "print([tokenizer.id_to_token(token) for token in tokenizer.encode(\"Das ist ein Beispiel\").ids])\n",
    "# Should return something like ['<s>', 'Das', 'ist', 'ein', 'Beispiel', '</s>']\n",
    "\n",
    "print(tokenizer.token_to_id(\"</s>\"))\n",
    "# Should return a valid token ID for '</s>'\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(\"Das ist ein Beispiel.\").ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = tokenizer.token_to_id('<pad>')\n",
    "BOS_TOKEN_ID = tokenizer.token_to_id('<s>')\n",
    "EOS_TOKEN_ID = tokenizer.token_to_id('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOS_TOKEN_ID, PAD_TOKEN_ID, EOS_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, bos_token_id: int = BOS_TOKEN_ID, eos_token_id: int = EOS_TOKEN_ID ,pad_token_id:int = PAD_TOKEN_ID, max_length: int = 512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.bos = bos_token_id\n",
    "        self.eos = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.dataset[idx]['translation']['de']\n",
    "        tgt_sentence = self.dataset[idx]['translation']['en']\n",
    "\n",
    "        # tokenize the source and target\n",
    "        src_tokens = self.tokenizer.encode(src_sentence).ids\n",
    "        tgt_tokens = self.tokenizer.encode(tgt_sentence).ids\n",
    "\n",
    "        # pad and truncate\n",
    "        src_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(src_tokens)))\n",
    "        tgt_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(tgt_tokens)))\n",
    "\n",
    "        # # create attention masks\n",
    "        # src_mask = (src_tokens != self.pad_token_id).int()\n",
    "        # tgt_mask = (src_tokens != self.pad_token_id).int()\n",
    "\n",
    "        # # create look ahead mask\n",
    "        # look_ahead_mask = self.create_causal_mask(len(tgt_tokens))\n",
    "\n",
    "\n",
    "        return {\n",
    "            'src_sentence': src_sentence, \n",
    "            'tgt_sentence': tgt_sentence, \n",
    "            'src_tokens': src_tokens,\n",
    "            'tgt_tokens': tgt_tokens,\n",
    "            # 'src_mask': src_mask,\n",
    "            # 'tgt_mask': tgt_mask,\n",
    "            # 'look_ahead_mask': look_ahead_mask,\n",
    "            # 'combined_mask': tgt_mask & look_ahead_mask\n",
    "        }\n",
    "\n",
    "    def pad_and_truncate(self, tokens):\n",
    "        if len(tokens) < self.max_length:\n",
    "            tokens = tokens + [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def add_special_tokens(self, tokens):\n",
    "        return [self.bos] + tokens + [self.eos]\n",
    "\n",
    "    def create_causal_mask(self, size):\n",
    "        # create an lower triangular matrix for the purposes of look ahead masking\n",
    "        return torch.tril(torch.ones(size, size)).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_translation_ds = TranslationDataset(small_train_dataset, tokenizer=tokenizer, pad_token_id=PAD_TOKEN_ID, max_length=30)\n",
    "small_translation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_translation_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function for handling masks\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    return torch.tril(torch.ones(attn_shape)).type(torch.uint8)  # Shape: (1, seq_length, seq_length)\n",
    "\n",
    "def create_std_mask(tgt, pad_token_id = PAD_TOKEN_ID):\n",
    "    tgt_mask = (tgt != pad_token_id).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & create_causal_mask(tgt.size(-1))\n",
    "    return tgt_mask\n",
    "    \n",
    "def collate_fn(batch, pad_token_id = PAD_TOKEN_ID):\n",
    "    src_batch = torch.stack([item['src_tokens'] for item in batch])\n",
    "    tgt_batch = torch.stack([item['tgt_tokens'] for item in batch])\n",
    "\n",
    "    # create source masks\n",
    "    src_mask = (src_batch != pad_token_id).unsqueeze(-2).int() # shape: (bs, seq_length, 1)\n",
    "    tgt = tgt_batch[:, :-1]\n",
    "    tgt_y = tgt_batch[:, 1:]\n",
    "    tgt_mask = create_std_mask(tgt, pad_token_id=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'src_tokens': src_batch,\n",
    "        'tgt_input': tgt, \n",
    "        'tgt_output': tgt_y,\n",
    "        'src_mask': src_mask, \n",
    "        'tgt_mask': tgt_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dl = DataLoader(small_translation_ds, collate_fn=collate_fn, batch_size=4)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each layer step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def scaled_dpa(query, key, value, mask=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Implements scaled dot product attention.\n",
    "    Args:\n",
    "        query: (batch_size, seq_length, dim_k)\n",
    "        key: (batch_size, seq_length, dim_k)\n",
    "        value: (batch_size, seq_length, dim_v)\n",
    "        mask: (batch_size, seq_length) or None\n",
    "        verbose: Boolean default False\n",
    "    Returns:\n",
    "        attention_output: (batch_size, seq_length, dim_v)\n",
    "        attention_weights: (batch_size, seq_length, seq_length)\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)# (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Scores shape: {scores.shape}\")\n",
    "    \n",
    "    # apply the mask if necessary\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    \n",
    "    # apply softmax to get attention_weights\n",
    "    attention_weights = F.softmax(scores, dim=-1) # (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Attention output shape: {output.shape}\")\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "\n",
    "# example scores\n",
    "scores = torch.rand(batch_size, seq_length, seq_length)\n",
    "print(scores)\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "print(mask)\n",
    "\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask.shape)\n",
    "\n",
    "scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scaled dpa\n",
    "# Example of how to use scaled_dpa with random tensors\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "mask = mask.unsqueeze(1).to(device)\n",
    "\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Test scaled_dpa\n",
    "output, attention_weights = scaled_dpa(query, key, value, mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with mask\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a padding mask (1 for valid tokens, 0 for padding tokens).\n",
    "    seq: Tensor of shape (batch_size, seq_length)\n",
    "    \"\"\"\n",
    "    return (seq != 0).unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(size, size)).type(torch.uint8)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Test scaled_dpa with padding and causal masks\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 1\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0, 0]]).to(device)  # Example with 2 padding tokens\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (for testing both padding and causal masking together)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0).to(device)\n",
    "\n",
    "# Test scaled_dpa with the mask\n",
    "output, attention_weights = scaled_dpa(query, key, value, combined_mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout=0.1, verbose=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Num heads: {num_heads}\")\n",
    "            print(f\"Embedding dimension: {d_model}\")\n",
    "            print(f\"per head dimension: {self.d_k}\")\n",
    "    \n",
    "        # linear layers to project the inputs to query, key, and value\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout) \n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query shape is bs, seq_length, d_model\n",
    "        # key shape is bs, seq_length, d_model\n",
    "        # value shape is bs, d_model, d_model\n",
    "        batch_size = query.size(0)\n",
    "        seq_length = query.size(1)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # Same mask applied to all heads. \n",
    "\n",
    "        if self.verbose and mask is not None:\n",
    "            print(f\"Mask shape (after unsqueezing at 1): {mask.shape}\")\n",
    "\n",
    "        # apply linear layers\n",
    "        query = self.query_linear(query)   # shape bs, seq_length, d_model\n",
    "        key = self.key_linear(key) #shape: bs, seq_length, d_model\n",
    "        value = self.value_linear(value) # shape: bs, d_model, d_model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Query shape: {query.shape}\")\n",
    "            print(f\"Key shape: {key.shape}\")\n",
    "            print(f\"Value shape: {value.shape}\")\n",
    "        \n",
    "        # reshape and split into multiple heads\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) #(bs, num_heads, seq_length, d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Shapes after projections for query, key, value...\")\n",
    "            print(f\"{query.shape}, {key.shape}, {value.shape}\")\n",
    "\n",
    "        attn_output, attn_weights = scaled_dpa(query, key, value, mask, verbose = self.verbose)\n",
    "\n",
    "        # we've separated the query key and value into separate heads and then computed the scaled dot-product attention for each head.\n",
    "        # Now we must put them back together. \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Attention output shape after concat: {attn_output.shape}\")\n",
    "\n",
    "        # apply the final linear layer transformation\n",
    "        output = self.output_linear(attn_output)\n",
    "        if self.verbose:\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing how the transpsoe works\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model).to(device)\n",
    "print(query.shape)\n",
    "print(f\"unchanged query: {query}\")\n",
    "\n",
    "query = query.view(batch_size, -1, num_heads, d_k)\n",
    "print(f\"prior to transpose query: {query}\")\n",
    "\n",
    "query = query.transpose(1, 2)\n",
    "print(f\"transposed query: {query}\")\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model)\n",
    "\n",
    "query = query.view(batch_size, num_heads, -1, d_k)\n",
    "print(f\"Directly reshaping query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MultiHeadAttention with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "seq_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# No mask for now (can add later)\n",
    "mask = None\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention\n",
    "output, attention_weights = multihead_attn(query, key, value, mask)\n",
    "\n",
    "print(\"Multi-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MultiHeadAttention with a padding mask and causal mask\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example with 1 padding token\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (bitwise AND to use both padding and causal masks)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0)\n",
    "combined_mask.to(device)\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention with a mask\n",
    "output, attention_weights = multihead_attn(query, key, value, combined_mask)\n",
    "\n",
    "print(\"\\nMulti-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_ff: int, d_model: int, dropout: float = 0.1):\n",
    "        super(PositionwiseFFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.verbose:\n",
    "            print(f\"Input to Encoder Layer: {x.shape}\")\n",
    "        \n",
    "        # Multi-head attention with residual connection and layer normalization\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        if self.verbose:\n",
    "            print(f\"attn_output shape: {attn_output.shape}\")\n",
    "        out1 = self.layernorm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ffn_output))  # Fixed: add out1, not x\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Output from Encoder Layer: {out2.shape}\")\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EncoderLayer with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "d_ff = 16\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random input sequence\n",
    "x = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random padding mask (e.g., if needed)\n",
    "padding_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Example with padding\n",
    "print(f\"Padding mask: {padding_mask.int()}\")\n",
    "\n",
    "# Create EncoderLayer object\n",
    "encoder_layer = EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, verbose=True).to(device)\n",
    "\n",
    "# Pass the input through the encoder layer\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "\n",
    "print(\"\\nOutput from Encoder Layer:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a padding mask (0 indicates padding)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example sequence with padding\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)\n",
    "\n",
    "# Test EncoderLayer with padding mask\n",
    "encoder_layer = EncoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "x = torch.rand(1, 4, 8).to(device)  # Random input sequence\n",
    "\n",
    "# Pass through the encoder layer with the mask\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "print(\"Output from encoder layer with padding mask:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.src_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape x: {x.shape}\")\n",
    "            print(f\"Encoder output shape: {enc_output.shape}\\n\")\n",
    "        # masked self-attention over the target (with look-ahead mask)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Passing through self-attention\")\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.layernorms[0](x + self.dropout(self_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nPassing Through encoder-decoder attention\")\n",
    "        # encoder-decoder attention over the encoder output (attend to source)\n",
    "        enc_dec_attn_output, _ = self.src_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.layernorms[1](x + self.dropout(enc_dec_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal feedforward of layer\")\n",
    "        # feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorms[2](x + self.dropout(ffn_output))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nOutput shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_length):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(seq_length, seq_length)).type(torch.uint8)  # Shape: (seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random input sequence for target (decoder input)\n",
    "tgt = torch.rand(1, 4, 8).to(device)  # (batch_size=1, seq_length=4, d_model=8)\n",
    "\n",
    "# Random encoder output (assuming same dimensions for simplicity)\n",
    "enc_output = torch.rand(1, 4, 8).to(device)\n",
    "\n",
    "# Create masks\n",
    "tgt_mask = create_causal_mask(seq_length=4).unsqueeze(0).to(device)  # Causal mask for target\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Padding mask for source\n",
    "\n",
    "# Initialize the decoder layer\n",
    "decoder_layer = DecoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "\n",
    "# Pass through the decoder layer\n",
    "output = decoder_layer(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "print(\"Output from decoder layer:\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # final layer normalization layer\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input of shape: {x.shape}\")\n",
    "        \n",
    "        for i, block in enumerate(self.encoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------ Passing Through Encoder block {i + 1} ----------------\")\n",
    "            \n",
    "            x = block(x, mask=src_mask)\n",
    "\n",
    "        # apply final layer normalization\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal output shape is: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random input sequence (batch_size=1, seq_length=4, d_model=8)\n",
    "src = torch.rand(1, 4, 8)\n",
    "\n",
    "# Create a padding mask for the source sequence\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]]))\n",
    "\n",
    "# Initialize the encoder with 2 blocks for testing\n",
    "encoder = Encoder(num_blocks=2, num_heads=2, d_model=8, d_ff=16, dropout=0.1, max_length=30, verbose=True)\n",
    "\n",
    "# Pass through the encoder\n",
    "output = encoder(src, src_mask=src_mask)\n",
    "print(\"Final output from encoder:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with actual examples\n",
    "batch_size = 4\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(batch.keys())\n",
    "    src_tokens = batch['src_tokens'].to(device)  # The tokenized source sentences\n",
    "    tgt_input = batch['tgt_input'].to(device)  # The tokenized target sentences\n",
    "    tgt_output = batch['tgt_output'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "    tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "    print(f\"Source tokens: {src_tokens.shape}\")\n",
    "    print(f\"tgt_input: {tgt_input.shape}\")\n",
    "    print(f\"tgt_output: {tgt_output.shape}\")\n",
    "    print(f\"src_mask: {src_mask.shape}\")\n",
    "    print(f\"tgt_mask: {tgt_mask.shape}\")\n",
    "\n",
    "    break  # Just getting the first batch for demonstration\n",
    "\n",
    "encoder = Encoder(num_blocks=6, num_heads=8, d_model=512, d_ff=2048, verbose=True).to(device)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), 512).to(device)\n",
    "pos_encoder = PositionalEncoding(512, dropout=0.1, max_len=512).to(device)\n",
    "\n",
    "for batch in small_dl:\n",
    "    src_tokens = batch['src_tokens'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "\n",
    "    print(f\"\\nSource token shape: {src_tokens.shape}\")\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------- Passing Through Decoder Block {i+1} ----------------\")\n",
    "            tgt = block(tgt, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.layernorm(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Mock data for testing\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "d_ff = 2048\n",
    "\n",
    "# Random embedded target tokens (already embedded, just mock data)\n",
    "tgt_embed = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Random encoder output (to simulate the output from the encoder)\n",
    "enc_output = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Create padding mask (mock data, assume no padding tokens for simplicity)\n",
    "src_mask = torch.ones(batch_size, 1, seq_length)  # Shape: (batch_size, 1, seq_length)\n",
    "print(src_mask.shape)\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones(batch_size, seq_length, seq_length))\n",
    "print(tgt_mask.shape)\n",
    "\n",
    "\n",
    "# Initialize the decoder without embedding\n",
    "decoder = Decoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=0.1, verbose=True)\n",
    "\n",
    "# Pass the mock data through the decoder\n",
    "decoder_output = decoder(tgt_embed, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to instantiate an encoder and decoder class and string them together to confirm that everything works together. Than we will abstract and create an Encoder-Decoder sequence to sequence model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define the linear + softmax step for generating token probabilities.\n",
    "        Layer projects vector on to vocab space and then applys a log_softmax. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 30\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len)\n",
    "\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target input tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "\n",
    "    src_tokens = batch['src_tokens']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_input = batch['tgt_input']\n",
    "    tgt_output = batch['tgt_output']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_mask = batch['tgt_mask']\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "\n",
    "    print(f\"Encoder output: {encoder_output.shape}\")\n",
    "\n",
    "    tgt_embed = embedding(tgt_input)\n",
    "    tgt_embed = pos_encoder(tgt_embed)\n",
    "    dec_output = decoder(tgt=tgt_embed, enc_output = encoder_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "    output = generator(dec_output)\n",
    "    predicted_tokens = torch.argmax(output, dim=-1)\n",
    "\n",
    "    print(output)\n",
    "    print(tgt_output)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(predicted_tokens.shape)\n",
    "    print(tgt_output.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we abstract the above into a EncoderDecoder class. \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, generator: Generator, embedding: nn.Embedding, pos_encoder: PositionalEncoding, verbose: bool = False):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "        self.embedding = embedding\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, src_tokens, tgt_input, src_mask, tgt_mask):\n",
    "        # Encoder\n",
    "        src_embed = self.embedding(src_tokens)\n",
    "        src_embed = self.pos_encoder(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_embed = self.embedding(tgt_input)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        dec_output = self.decoder(tgt=tgt_embed, enc_output=encoder_output, src_mask=src_mask, tgt_mask = tgt_mask)\n",
    "\n",
    "        output_log_probs = self.generator(dec_output)\n",
    "\n",
    "        return output_log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_ds = TranslationDataset(dataset['train'].shuffle().select(range(20000)), tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)\n",
    "val_ds = TranslationDataset(dataset['validation'], tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate models\n",
    "\n",
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 512\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model).to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len).to(device)\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder, generator, embedding, pos_encoder, verbose=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate scheduler, following `Attention is All You Need` for now. \n",
    "# lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n",
    "warmup_steps = 4000\n",
    "\n",
    "def get_lr(step_num):\n",
    "    return d_model ** -0.5 * min(step_num ** -0.5, step_num * warmup_steps ** -1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# training loop\n",
    "# optimizer and criterion\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "step_num = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for batch in tqdm(train_dl):\n",
    "        step_num += 1\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = get_lr(step_num)\n",
    "\n",
    "        src_tokens = batch['src_tokens'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        src_mask = batch['src_mask'].to(device)\n",
    "        tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "        # print(src_tokens.device)\n",
    "        # print(src_mask.device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_logits = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = criterion(output_logits.view(-1, output_logits.size(-1)), tgt_output.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {avg_loss: .4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl):\n",
    "            src_tokens = batch['src_tokens'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_output = batch['tgt_output'].to(device)\n",
    "            src_mask = batch['src_mask'].to(device)\n",
    "            tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "            output = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dl)\n",
    "    print(F\"Epoch {epoch + 1}/{num_epochs}, Average validation loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if inference works\n",
    "num_examples = 10\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(val_ds[i])\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model: EncoderDecoder, src_tokens: torch.Tensor, tokenizer: ByteLevelBPETokenizer = tokenizer, pad_token_id: int = PAD_TOKEN_ID, max_len=100):\n",
    "    model.eval()\n",
    "\n",
    "    # embed the source tokens and create the src_mask\n",
    "    src_tokens = src_tokens.unsqueeze(0).to(device)\n",
    "    src_mask = (src_tokens != pad_token_id).unsqueeze(0).to(device) # shape: (1, seq_length)\n",
    "\n",
    "    src_embed = model.embedding(src_tokens)\n",
    "    src_embed = model.pos_encoder(src_embed)\n",
    "\n",
    "    # store encoder hidden states for the src_tokens\n",
    "    encoder_output = model.encoder(src_embed, src_mask)\n",
    "\n",
    "    # initizlie target sentence with BOS token\n",
    "    tgt_tokens = torch.tensor([BOS_TOKEN_ID], dtype=torch.long).to(device)\n",
    "\n",
    "    # Autoregressive loop to generate sentence\n",
    "    for _ in range(max_len):\n",
    "        # create target mask\n",
    "        tgt_seq_len = tgt_tokens.size(0)\n",
    "        tgt_mask = torch.tril(torch.ones(1, tgt_seq_len, tgt_seq_len)).to(device)\n",
    "        #print(f\"target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "        #print(f\"Tokens at beginning: {tgt_tokens}\")\n",
    "        tgt_embed = model.embedding(tgt_tokens).unsqueeze(0)\n",
    "        #print(f\"Token embeddings shape: {tgt_embed.shape}\")\n",
    "        tgt_embed = model.pos_encoder(tgt_embed)\n",
    "\n",
    "        output_logits = model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "        output_log_probs = model.generator(output_logits)\n",
    "        # print(output_log_probs.shape)\n",
    "        # print(output_log_probs)\n",
    "        next_token = torch.argmax(output_log_probs[:, -1, :], dim=-1)\n",
    "        # print(next_token.shape)\n",
    "        #print(f\"Next token: {next_token.item()}\")\n",
    "        #print(tgt_tokens.shape)\n",
    "        # append next\n",
    "        tgt_tokens = torch.cat([tgt_tokens, next_token])\n",
    "\n",
    "        if next_token.item() == EOS_TOKEN_ID or tgt_tokens.size(0) >= 50:\n",
    "            break\n",
    "\n",
    "        #print(f\"Resulting Target Tokens: {tgt_tokens}\")\n",
    "\n",
    "    print(f\"Source sentence: {tokenizer.decode([num for num in src_tokens.squeeze(0).tolist() if num != pad_token_id], skip_special_tokens=True)}\")\n",
    "    print(f\"Translation: {tokenizer.decode(tgt_tokens.tolist())}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
