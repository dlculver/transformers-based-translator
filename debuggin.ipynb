{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import components\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download and inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WMT14 dataset for German-English translation\n",
    "dataset = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 4508785\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 3003\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'de': 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.',\n",
       "  'en': \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a very small segment for experimentation\n",
    "# Take a small subset for experimentation\n",
    "small_train_dataset = dataset['train'].select(range(20))\n",
    "small_val_dataset = dataset['validation'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are following the original `Attention is all you need paper` we will use Byte-Pair Encoding\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"bpe_tokenizer/vocab.json\",\n",
    "    \"bpe_tokenizer/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[789, 423, 328, 3010, 18]\n",
      "['Das', 'Ġist', 'Ġein', 'ĠBeispiel']\n",
      "2\n",
      "Das ist ein Beispiel.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Das ist ein Beispiel.\").ids)\n",
    "\n",
    "print([tokenizer.id_to_token(token) for token in tokenizer.encode(\"Das ist ein Beispiel\").ids])\n",
    "# Should return something like ['<s>', 'Das', 'ist', 'ein', 'Beispiel', '</s>']\n",
    "\n",
    "print(tokenizer.token_to_id(\"</s>\"))\n",
    "# Should return a valid token ID for '</s>'\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(\"Das ist ein Beispiel.\").ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_ID = tokenizer.token_to_id('<pad>')\n",
    "BOS_TOKEN_ID = tokenizer.token_to_id('<s>')\n",
    "EOS_TOKEN_ID = tokenizer.token_to_id('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, bos_token_id: int = BOS_TOKEN_ID, eos_token_id: int = EOS_TOKEN_ID ,pad_token_id:int = PAD_TOKEN_ID, max_length: int = 512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.bos = bos_token_id\n",
    "        self.eos = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_sentence = self.dataset[idx]['translation']['de']\n",
    "        tgt_sentence = self.dataset[idx]['translation']['en']\n",
    "\n",
    "        # tokenize the source and target\n",
    "        src_tokens = self.tokenizer.encode(src_sentence).ids\n",
    "        tgt_tokens = self.tokenizer.encode(tgt_sentence).ids\n",
    "\n",
    "        # pad and truncate\n",
    "        src_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(src_tokens)))\n",
    "        tgt_tokens = torch.tensor(self.pad_and_truncate(self.add_special_tokens(tgt_tokens)))\n",
    "\n",
    "        # # create attention masks\n",
    "        # src_mask = (src_tokens != self.pad_token_id).int()\n",
    "        # tgt_mask = (src_tokens != self.pad_token_id).int()\n",
    "\n",
    "        # # create look ahead mask\n",
    "        # look_ahead_mask = self.create_causal_mask(len(tgt_tokens))\n",
    "\n",
    "\n",
    "        return {\n",
    "            'src_sentence': src_sentence, \n",
    "            'tgt_sentence': tgt_sentence, \n",
    "            'src_tokens': src_tokens,\n",
    "            'tgt_tokens': tgt_tokens,\n",
    "            # 'src_mask': src_mask,\n",
    "            # 'tgt_mask': tgt_mask,\n",
    "            # 'look_ahead_mask': look_ahead_mask,\n",
    "            # 'combined_mask': tgt_mask & look_ahead_mask\n",
    "        }\n",
    "\n",
    "    def pad_and_truncate(self, tokens):\n",
    "        if len(tokens) < self.max_length:\n",
    "            tokens = tokens + [self.pad_token_id] * (self.max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def add_special_tokens(self, tokens):\n",
    "        return [self.bos] + tokens + [self.eos]\n",
    "\n",
    "    def create_causal_mask(self, size):\n",
    "        # create an lower triangular matrix for the purposes of look ahead masking\n",
    "        return torch.tril(torch.ones(size, size)).type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TranslationDataset at 0x292ced610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_translation_ds = TranslationDataset(small_train_dataset, tokenizer=tokenizer, pad_token_id=PAD_TOKEN_ID, max_length=30)\n",
    "small_translation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_sentence': 'Wiederaufnahme der Sitzungsperiode',\n",
       " 'tgt_sentence': 'Resumption of the session',\n",
       " 'src_tokens': tensor([    0, 23062, 17719,   319, 26699,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'tgt_tokens': tensor([    0,  8859, 27958,   304,   280,  9974,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_translation_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function for handling masks\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    return torch.tril(torch.ones(attn_shape)).type(torch.uint8)  # Shape: (1, seq_length, seq_length)\n",
    "\n",
    "def create_std_mask(tgt, pad_token_id = PAD_TOKEN_ID):\n",
    "    tgt_mask = (tgt != pad_token_id).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & create_causal_mask(tgt.size(-1))\n",
    "    return tgt_mask\n",
    "    \n",
    "def collate_fn(batch, pad_token_id = PAD_TOKEN_ID):\n",
    "    src_batch = torch.stack([item['src_tokens'] for item in batch])\n",
    "    tgt_batch = torch.stack([item['tgt_tokens'] for item in batch])\n",
    "\n",
    "    # create source masks\n",
    "    src_mask = (src_batch != pad_token_id).unsqueeze(-2).int() # shape: (bs, seq_length, 1)\n",
    "    tgt = tgt_batch[:, :-1]\n",
    "    tgt_y = tgt_batch[:, 1:]\n",
    "    tgt_mask = create_std_mask(tgt, pad_token_id=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'src_tokens': src_batch,\n",
    "        'tgt_input': tgt, \n",
    "        'tgt_output': tgt_y,\n",
    "        'src_mask': src_mask, \n",
    "        'tgt_mask': tgt_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tokens: torch.Size([4, 30])\n",
      "Target tokens: torch.Size([4, 29])\n",
      "Target output tokens: torch.Size([4, 29])\n",
      "Source mask: torch.Size([4, 1, 30])\n",
      "Target mask: torch.Size([4, 29, 29])\n"
     ]
    }
   ],
   "source": [
    "small_dl = DataLoader(small_translation_ds, collate_fn=collate_fn, batch_size=4)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each layer step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def scaled_dpa(query, key, value, mask=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Implements scaled dot product attention.\n",
    "    Args:\n",
    "        query: (batch_size, seq_length, dim_k)\n",
    "        key: (batch_size, seq_length, dim_k)\n",
    "        value: (batch_size, seq_length, dim_v)\n",
    "        mask: (batch_size, seq_length) or None\n",
    "        verbose: Boolean default False\n",
    "    Returns:\n",
    "        attention_output: (batch_size, seq_length, dim_v)\n",
    "        attention_weights: (batch_size, seq_length, seq_length)\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)# (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Scores shape: {scores.shape}\")\n",
    "    \n",
    "    # apply the mask if necessary\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    \n",
    "    # apply softmax to get attention_weights\n",
    "    attention_weights = F.softmax(scores, dim=-1) # (bs, seq_length, seq_length)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Attention output shape: {output.shape}\")\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2889, 0.3645, 0.5073, 0.0601, 0.1955],\n",
      "         [0.4960, 0.7317, 0.2633, 0.5181, 0.3663],\n",
      "         [0.5461, 0.5656, 0.1713, 0.6215, 0.9109],\n",
      "         [0.1746, 0.3636, 0.8319, 0.7131, 0.2682],\n",
      "         [0.4274, 0.7795, 0.9693, 0.7363, 0.5564]],\n",
      "\n",
      "        [[0.9173, 0.4745, 0.6934, 0.0855, 0.6556],\n",
      "         [0.4114, 0.0378, 0.4886, 0.3944, 0.3892],\n",
      "         [0.0564, 0.7735, 0.9761, 0.2099, 0.1457],\n",
      "         [0.7007, 0.1584, 0.1256, 0.8583, 0.9621],\n",
      "         [0.7026, 0.4526, 0.0107, 0.3048, 0.5285]],\n",
      "\n",
      "        [[0.5153, 0.2527, 0.0817, 0.3672, 0.7689],\n",
      "         [0.5652, 0.5058, 0.1088, 0.6928, 0.2403],\n",
      "         [0.0115, 0.8514, 0.6130, 0.9758, 0.5974],\n",
      "         [0.7339, 0.7190, 0.9975, 0.1526, 0.9539],\n",
      "         [0.8057, 0.1836, 0.2223, 0.7273, 0.5501]]])\n",
      "tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "torch.Size([3, 1, 5])\n",
      "tensor([[[0.2889, 0.3645, 0.5073,   -inf,   -inf],\n",
      "         [0.4960, 0.7317, 0.2633,   -inf,   -inf],\n",
      "         [0.5461, 0.5656, 0.1713,   -inf,   -inf],\n",
      "         [0.1746, 0.3636, 0.8319,   -inf,   -inf],\n",
      "         [0.4274, 0.7795, 0.9693,   -inf,   -inf]],\n",
      "\n",
      "        [[0.9173, 0.4745,   -inf,   -inf,   -inf],\n",
      "         [0.4114, 0.0378,   -inf,   -inf,   -inf],\n",
      "         [0.0564, 0.7735,   -inf,   -inf,   -inf],\n",
      "         [0.7007, 0.1584,   -inf,   -inf,   -inf],\n",
      "         [0.7026, 0.4526,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.5153, 0.2527, 0.0817, 0.3672, 0.7689],\n",
      "         [0.5652, 0.5058, 0.1088, 0.6928, 0.2403],\n",
      "         [0.0115, 0.8514, 0.6130, 0.9758, 0.5974],\n",
      "         [0.7339, 0.7190, 0.9975, 0.1526, 0.9539],\n",
      "         [0.8057, 0.1836, 0.2223, 0.7273, 0.5501]]])\n"
     ]
    }
   ],
   "source": [
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "\n",
    "# example scores\n",
    "scores = torch.rand(batch_size, seq_length, seq_length)\n",
    "print(scores)\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "print(mask)\n",
    "\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask.shape)\n",
    "\n",
    "scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([3, 5, 4])\n",
      "Mask shape: torch.Size([3, 1, 5])\n",
      "Scores shape: torch.Size([3, 5, 5])\n",
      "Attention weights shape: torch.Size([3, 5, 5])\n",
      "Attention output shape: torch.Size([3, 5, 4])\n",
      "Attention Output:\n",
      " tensor([[[0.2269, 0.3303, 0.5288, 0.5487],\n",
      "         [0.2299, 0.3100, 0.5195, 0.5397],\n",
      "         [0.2408, 0.2945, 0.5300, 0.5479],\n",
      "         [0.2357, 0.3024, 0.5256, 0.5445],\n",
      "         [0.2238, 0.3349, 0.5260, 0.5466]],\n",
      "\n",
      "        [[0.4412, 0.2081, 0.4441, 0.5420],\n",
      "         [0.4452, 0.2067, 0.4396, 0.5500],\n",
      "         [0.4453, 0.2067, 0.4395, 0.5501],\n",
      "         [0.4298, 0.2120, 0.4568, 0.5192],\n",
      "         [0.4321, 0.2112, 0.4542, 0.5239]],\n",
      "\n",
      "        [[0.3537, 0.3539, 0.4510, 0.4363],\n",
      "         [0.3498, 0.3251, 0.4349, 0.3921],\n",
      "         [0.3582, 0.3571, 0.4370, 0.4307],\n",
      "         [0.3551, 0.3546, 0.4511, 0.4361],\n",
      "         [0.3521, 0.3384, 0.4405, 0.4132]]], device='mps:0')\n",
      "Attention Weights:\n",
      " tensor([[[0.4430, 0.3145, 0.2425, 0.0000, 0.0000],\n",
      "         [0.4025, 0.3140, 0.2835, 0.0000, 0.0000],\n",
      "         [0.3771, 0.3340, 0.2889, 0.0000, 0.0000],\n",
      "         [0.3903, 0.3249, 0.2848, 0.0000, 0.0000],\n",
      "         [0.4507, 0.3090, 0.2404, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5182, 0.4818, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5286, 0.4714, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5289, 0.4711, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4886, 0.5114, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4946, 0.5054, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1502, 0.2351, 0.2505, 0.1450, 0.2193],\n",
      "         [0.1782, 0.1981, 0.2202, 0.1712, 0.2323],\n",
      "         [0.1604, 0.2306, 0.2472, 0.1659, 0.1960],\n",
      "         [0.1519, 0.2418, 0.2400, 0.1565, 0.2099],\n",
      "         [0.1683, 0.2096, 0.2428, 0.1514, 0.2280]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# test scaled dpa\n",
    "# Example of how to use scaled_dpa with random tensors\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 3\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "\n",
    "# Optional mask\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0, 0], \n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "mask = mask.unsqueeze(1).to(device)\n",
    "\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Test scaled_dpa\n",
    "output, attention_weights = scaled_dpa(query, key, value, mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: torch.Size([1, 5, 5])\n",
      "Attention weights shape: torch.Size([1, 1, 5, 5])\n",
      "Attention output shape: torch.Size([1, 1, 5, 4])\n",
      "Attention Output:\n",
      " tensor([[[[0.0599, 0.9591, 0.5854, 0.6830],\n",
      "          [0.0922, 0.6035, 0.3744, 0.4882],\n",
      "          [0.2372, 0.7045, 0.3788, 0.3680],\n",
      "          [0.2349, 0.7237, 0.3904, 0.3791],\n",
      "          [0.2515, 0.7268, 0.3861, 0.3616]]]], device='mps:0')\n",
      "Attention Weights:\n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4658, 0.5342, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3558, 0.3557, 0.2885, 0.0000, 0.0000],\n",
      "          [0.3855, 0.3269, 0.2875, 0.0000, 0.0000],\n",
      "          [0.3615, 0.3194, 0.3191, 0.0000, 0.0000]]]], device='mps:0')\n",
      "Padding Mask:\n",
      " tensor([[[[ True,  True,  True, False, False]]]], device='mps:0')\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1]], device='mps:0', dtype=torch.uint8)\n",
      "Combined Mask:\n",
      " tensor([[[[1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 0, 0]]]], device='mps:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# testing with mask\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Creates a padding mask (1 for valid tokens, 0 for padding tokens).\n",
    "    seq: Tensor of shape (batch_size, seq_length)\n",
    "    \"\"\"\n",
    "    return (seq != 0).unsqueeze(1).unsqueeze(2)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(size, size)).type(torch.uint8)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Test scaled_dpa with padding and causal masks\n",
    "\n",
    "# Batch size = 1, Sequence length = 5, Embedding dimension = 4 (d_k)\n",
    "batch_size = 1\n",
    "seq_length = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Random queries, keys, and values\n",
    "query = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "key = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "value = torch.rand(batch_size, seq_length, embedding_dim).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0, 0]]).to(device)  # Example with 2 padding tokens\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (for testing both padding and causal masking together)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0).to(device)\n",
    "\n",
    "# Test scaled_dpa with the mask\n",
    "output, attention_weights = scaled_dpa(query, key, value, combined_mask, verbose=True)\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, dropout=0.1, verbose=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Num heads: {num_heads}\")\n",
    "            print(f\"Embedding dimension: {d_model}\")\n",
    "            print(f\"per head dimension: {self.d_k}\")\n",
    "    \n",
    "        # linear layers to project the inputs to query, key, and value\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout) \n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query shape is bs, seq_length, d_model\n",
    "        # key shape is bs, seq_length, d_model\n",
    "        # value shape is bs, d_model, d_model\n",
    "        batch_size = query.size(0)\n",
    "        seq_length = query.size(1)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # Same mask applied to all heads. \n",
    "\n",
    "        if self.verbose and mask is not None:\n",
    "            print(f\"Mask shape (after unsqueezing at 1): {mask.shape}\")\n",
    "\n",
    "        # apply linear layers\n",
    "        query = self.query_linear(query)   # shape bs, seq_length, d_model\n",
    "        key = self.key_linear(key) #shape: bs, seq_length, d_model\n",
    "        value = self.value_linear(value) # shape: bs, d_model, d_model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Query shape: {query.shape}\")\n",
    "            print(f\"Key shape: {key.shape}\")\n",
    "            print(f\"Value shape: {value.shape}\")\n",
    "        \n",
    "        # reshape and split into multiple heads\n",
    "        query = query.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        key = key.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # (bs, num_heads, seq_length, d_k)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) #(bs, num_heads, seq_length, d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Shapes after projections for query, key, value...\")\n",
    "            print(f\"{query.shape}, {key.shape}, {value.shape}\")\n",
    "\n",
    "        attn_output, attn_weights = scaled_dpa(query, key, value, mask, verbose = self.verbose)\n",
    "\n",
    "        # we've separated the query key and value into separate heads and then computed the scaled dot-product attention for each head.\n",
    "        # Now we must put them back together. \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Attention output shape after concat: {attn_output.shape}\")\n",
    "\n",
    "        # apply the final linear layer transformation\n",
    "        output = self.output_linear(attn_output)\n",
    "        if self.verbose:\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "unchanged query: tensor([[[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
      "         [17, 18, 19, 20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29, 30, 31, 32]]], device='mps:0')\n",
      "prior to transpose query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]],\n",
      "\n",
      "         [[17, 18, 19, 20],\n",
      "          [21, 22, 23, 24]],\n",
      "\n",
      "         [[25, 26, 27, 28],\n",
      "          [29, 30, 31, 32]]]], device='mps:0')\n",
      "transposed query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [17, 18, 19, 20],\n",
      "          [25, 26, 27, 28]],\n",
      "\n",
      "         [[ 5,  6,  7,  8],\n",
      "          [13, 14, 15, 16],\n",
      "          [21, 22, 23, 24],\n",
      "          [29, 30, 31, 32]]]], device='mps:0')\n",
      "torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# visualizing how the transpsoe works\n",
    "batch_size = 1\n",
    "seq_length = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model).to(device)\n",
    "print(query.shape)\n",
    "print(f\"unchanged query: {query}\")\n",
    "\n",
    "query = query.view(batch_size, -1, num_heads, d_k)\n",
    "print(f\"prior to transpose query: {query}\")\n",
    "\n",
    "query = query.transpose(1, 2)\n",
    "print(f\"transposed query: {query}\")\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directly reshaping query: tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]],\n",
      "\n",
      "         [[17, 18, 19, 20],\n",
      "          [21, 22, 23, 24],\n",
      "          [25, 26, 27, 28],\n",
      "          [29, 30, 31, 32]]]])\n"
     ]
    }
   ],
   "source": [
    "query = torch.arange(1, seq_length*d_model + 1).view(batch_size, seq_length, d_model)\n",
    "\n",
    "query = query.view(batch_size, num_heads, -1, d_k)\n",
    "print(f\"Directly reshaping query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 8\n",
      "Embedding dimension: 64\n",
      "per head dimension: 8\n",
      "Query shape: torch.Size([1, 5, 64])\n",
      "Key shape: torch.Size([1, 5, 64])\n",
      "Value shape: torch.Size([1, 5, 64])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 8, 5, 8]), torch.Size([1, 8, 5, 8]), torch.Size([1, 8, 5, 8])\n",
      "Scores shape: torch.Size([1, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([1, 8, 5, 5])\n",
      "Attention output shape: torch.Size([1, 8, 5, 8])\n",
      "Attention output shape after concat: torch.Size([1, 5, 64])\n",
      "Output shape: torch.Size([1, 5, 64])\n",
      "Multi-Head Attention Output:\n",
      " tensor([[[ 0.2293, -0.1196, -0.4825,  0.0844, -0.1668,  0.0588, -0.0671,\n",
      "           0.1175,  0.3363, -0.0163,  0.1162,  0.1029, -0.0682,  0.2925,\n",
      "           0.0719,  0.0330,  0.1565, -0.2045, -0.2230, -0.0716,  0.1627,\n",
      "          -0.0956, -0.0742,  0.1855,  0.0179,  0.1026,  0.1620, -0.0013,\n",
      "           0.2692, -0.2667,  0.1745,  0.0689,  0.1884,  0.3444,  0.2481,\n",
      "          -0.0640,  0.1578, -0.0885, -0.0780, -0.1707, -0.4239, -0.3066,\n",
      "          -0.1370,  0.0150,  0.1393,  0.1385, -0.0711, -0.0540, -0.3522,\n",
      "           0.1289,  0.1061,  0.1508, -0.0989, -0.0231, -0.1572,  0.1958,\n",
      "           0.2852,  0.1993, -0.0190, -0.0544, -0.2026,  0.1968,  0.2317,\n",
      "           0.2355],\n",
      "         [ 0.2309, -0.1204, -0.4822,  0.0841, -0.1685,  0.0578, -0.0680,\n",
      "           0.1182,  0.3381, -0.0177,  0.1147,  0.1026, -0.0684,  0.2920,\n",
      "           0.0700,  0.0336,  0.1578, -0.2050, -0.2246, -0.0691,  0.1634,\n",
      "          -0.0960, -0.0739,  0.1875,  0.0164,  0.1040,  0.1615, -0.0017,\n",
      "           0.2689, -0.2679,  0.1746,  0.0691,  0.1874,  0.3427,  0.2484,\n",
      "          -0.0642,  0.1567, -0.0877, -0.0756, -0.1721, -0.4220, -0.3064,\n",
      "          -0.1369,  0.0158,  0.1393,  0.1391, -0.0699, -0.0555, -0.3508,\n",
      "           0.1302,  0.1075,  0.1510, -0.1009, -0.0222, -0.1586,  0.1962,\n",
      "           0.2857,  0.2003, -0.0200, -0.0535, -0.2022,  0.1980,  0.2319,\n",
      "           0.2352],\n",
      "         [ 0.2308, -0.1210, -0.4821,  0.0805, -0.1701,  0.0585, -0.0684,\n",
      "           0.1192,  0.3378, -0.0179,  0.1164,  0.1042, -0.0694,  0.2938,\n",
      "           0.0697,  0.0336,  0.1574, -0.2050, -0.2227, -0.0687,  0.1659,\n",
      "          -0.0936, -0.0741,  0.1873,  0.0158,  0.1021,  0.1624, -0.0023,\n",
      "           0.2703, -0.2667,  0.1744,  0.0693,  0.1855,  0.3438,  0.2483,\n",
      "          -0.0639,  0.1572, -0.0898, -0.0754, -0.1728, -0.4226, -0.3044,\n",
      "          -0.1364,  0.0166,  0.1404,  0.1368, -0.0713, -0.0564, -0.3510,\n",
      "           0.1292,  0.1072,  0.1527, -0.0991, -0.0237, -0.1596,  0.1948,\n",
      "           0.2868,  0.2019, -0.0183, -0.0544, -0.2044,  0.1987,  0.2321,\n",
      "           0.2359],\n",
      "         [ 0.2316, -0.1195, -0.4816,  0.0843, -0.1663,  0.0581, -0.0689,\n",
      "           0.1186,  0.3359, -0.0159,  0.1156,  0.1039, -0.0681,  0.2920,\n",
      "           0.0718,  0.0335,  0.1562, -0.2051, -0.2231, -0.0714,  0.1626,\n",
      "          -0.0960, -0.0748,  0.1864,  0.0162,  0.1036,  0.1621, -0.0013,\n",
      "           0.2692, -0.2666,  0.1732,  0.0693,  0.1862,  0.3459,  0.2487,\n",
      "          -0.0633,  0.1568, -0.0887, -0.0782, -0.1714, -0.4248, -0.3074,\n",
      "          -0.1371,  0.0149,  0.1388,  0.1388, -0.0728, -0.0546, -0.3515,\n",
      "           0.1290,  0.1058,  0.1513, -0.0989, -0.0228, -0.1591,  0.1962,\n",
      "           0.2850,  0.2000, -0.0211, -0.0532, -0.2011,  0.1962,  0.2317,\n",
      "           0.2350],\n",
      "         [ 0.2324, -0.1214, -0.4797,  0.0818, -0.1670,  0.0586, -0.0690,\n",
      "           0.1187,  0.3368, -0.0162,  0.1164,  0.1045, -0.0690,  0.2929,\n",
      "           0.0696,  0.0340,  0.1574, -0.2031, -0.2229, -0.0707,  0.1628,\n",
      "          -0.0934, -0.0737,  0.1872,  0.0160,  0.1042,  0.1611, -0.0031,\n",
      "           0.2699, -0.2663,  0.1751,  0.0690,  0.1848,  0.3450,  0.2486,\n",
      "          -0.0641,  0.1572, -0.0869, -0.0787, -0.1723, -0.4234, -0.3068,\n",
      "          -0.1355,  0.0157,  0.1399,  0.1385, -0.0731, -0.0550, -0.3496,\n",
      "           0.1295,  0.1048,  0.1525, -0.0992, -0.0233, -0.1573,  0.1964,\n",
      "           0.2862,  0.2009, -0.0192, -0.0515, -0.2030,  0.1972,  0.2308,\n",
      "           0.2362]]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Attention Weights:\n",
      " torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadAttention with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 8\n",
    "d_model = 64\n",
    "seq_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# No mask for now (can add later)\n",
    "mask = None\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention\n",
    "output, attention_weights = multihead_attn(query, key, value, mask)\n",
    "\n",
    "print(\"Multi-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 4, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Multi-Head Attention Output:\n",
      " tensor([[[ 0.2070, -0.3633,  0.5263,  0.6589,  0.3929, -0.1892,  0.0745,\n",
      "          -0.3641],\n",
      "         [ 0.1263, -0.3127,  0.4872,  0.6978,  0.3566, -0.2125,  0.0865,\n",
      "          -0.2827],\n",
      "         [ 0.3505,  0.0285,  0.2862,  0.4353,  0.0862, -0.2340, -0.0065,\n",
      "           0.0970],\n",
      "         [ 0.3877,  0.0840,  0.3126,  0.3966,  0.0394, -0.1514,  0.0640,\n",
      "           0.1300]]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Attention Weights:\n",
      " tensor([[[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "           [0.4517, 0.5483, 0.0000, 0.0000],\n",
      "           [0.3194, 0.3712, 0.3093, 0.0000],\n",
      "           [0.3180, 0.3672, 0.3148, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "           [0.5114, 0.4886, 0.0000, 0.0000],\n",
      "           [0.3440, 0.3270, 0.3290, 0.0000],\n",
      "           [0.3431, 0.3222, 0.3347, 0.0000]]]]], device='mps:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Padding Mask:\n",
      " tensor([[[[ True,  True,  True, False]]]], device='mps:0')\n",
      "Causal Mask:\n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]], device='mps:0', dtype=torch.uint8)\n",
      "Combined Mask:\n",
      " tensor([[[[1, 0, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0]]]], device='mps:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Test MultiHeadAttention with a padding mask and causal mask\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random inputs for query, key, and value\n",
    "query = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "key = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "value = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random sequence with padding (0 represents padding token)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example with 1 padding token\n",
    "\n",
    "# Create a padding mask\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)  # Shape: (batch_size, 1, 1, seq_length)\n",
    "\n",
    "# Create a causal mask (look-ahead mask)\n",
    "causal_mask = create_causal_mask(seq_length).to(device)  # Shape: (seq_length, seq_length)\n",
    "\n",
    "# Combine the masks (bitwise AND to use both padding and causal masks)\n",
    "combined_mask = padding_mask & causal_mask.unsqueeze(0)\n",
    "combined_mask.to(device)\n",
    "\n",
    "# Create MultiHeadAttention object\n",
    "multihead_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, verbose=True).to(device)\n",
    "\n",
    "# Pass the inputs through multi-head attention with a mask\n",
    "output, attention_weights = multihead_attn(query, key, value, combined_mask)\n",
    "\n",
    "print(\"\\nMulti-Head Attention Output:\\n\", output)\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"Padding Mask:\\n\", padding_mask)\n",
    "print(\"Causal Mask:\\n\", causal_mask)\n",
    "print(\"Combined Mask:\\n\", combined_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, d_ff: int, d_model: int, dropout: float = 0.1):\n",
    "        super(PositionwiseFFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.verbose:\n",
    "            print(f\"Input to Encoder Layer: {x.shape}\")\n",
    "        \n",
    "        # Multi-head attention with residual connection and layer normalization\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        if self.verbose:\n",
    "            print(f\"attn_output shape: {attn_output.shape}\")\n",
    "        out1 = self.layernorm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ffn_output))  # Fixed: add out1, not x\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Output from Encoder Layer: {out2.shape}\")\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding mask: tensor([[[[1, 1, 1, 0]]]], device='mps:0', dtype=torch.int32)\n",
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input to Encoder Layer: torch.Size([1, 4, 8])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "attn_output shape: torch.Size([1, 4, 8])\n",
      "Output from Encoder Layer: torch.Size([1, 4, 8])\n",
      "\n",
      "Output from Encoder Layer:\n",
      " tensor([[[-0.7111,  1.5372, -1.4903,  0.1783,  0.3779, -1.3133,  0.6901,\n",
      "           0.7312],\n",
      "         [-1.7991,  1.2210, -0.5939, -0.6746,  0.7649,  0.0175, -0.2782,\n",
      "           1.3423],\n",
      "         [ 0.2757,  0.8118, -1.1125,  1.4158,  1.1203, -1.2995, -0.1905,\n",
      "          -1.0211],\n",
      "         [ 0.9409,  0.4888, -1.1315, -0.3195,  1.6914, -1.2897,  0.4794,\n",
      "          -0.8599]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test EncoderLayer with random inputs\n",
    "\n",
    "# Define parameters\n",
    "num_heads = 2\n",
    "d_model = 8\n",
    "d_ff = 16\n",
    "seq_length = 4\n",
    "batch_size = 1\n",
    "\n",
    "# Random input sequence\n",
    "x = torch.rand(batch_size, seq_length, d_model).to(device)\n",
    "\n",
    "# Create a random padding mask (e.g., if needed)\n",
    "padding_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Example with padding\n",
    "print(f\"Padding mask: {padding_mask.int()}\")\n",
    "\n",
    "# Create EncoderLayer object\n",
    "encoder_layer = EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, verbose=True).to(device)\n",
    "\n",
    "# Pass the input through the encoder layer\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "\n",
    "print(\"\\nOutput from Encoder Layer:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input to Encoder Layer: torch.Size([1, 4, 8])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "attn_output shape: torch.Size([1, 4, 8])\n",
      "Output from Encoder Layer: torch.Size([1, 4, 8])\n",
      "Output from encoder layer with padding mask: tensor([[[-0.7593, -0.3763, -0.1341, -1.4512,  0.6835,  0.4469, -0.4748,\n",
      "           2.0653],\n",
      "         [-0.5853, -0.7003,  0.2369, -1.6870,  0.4599, -0.1346,  0.4533,\n",
      "           1.9570],\n",
      "         [-0.6862,  0.3053,  0.3419, -2.3368,  0.0587,  0.5648,  0.8613,\n",
      "           0.8910],\n",
      "         [-0.1419, -0.2938, -0.0311, -1.5627, -0.7842,  0.1483,  0.5389,\n",
      "           2.1267]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a padding mask (0 indicates padding)\n",
    "src_tokens = torch.tensor([[1, 2, 3, 0]]).to(device)  # Example sequence with padding\n",
    "padding_mask = create_padding_mask(src_tokens).to(device)\n",
    "\n",
    "# Test EncoderLayer with padding mask\n",
    "encoder_layer = EncoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "x = torch.rand(1, 4, 8).to(device)  # Random input sequence\n",
    "\n",
    "# Pass through the encoder layer with the mask\n",
    "output = encoder_layer(x, mask=padding_mask)\n",
    "print(\"Output from encoder layer with padding mask:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose=False):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.src_attn = MultiHeadAttention(num_heads=num_heads, d_model=d_model, dropout=dropout, verbose=verbose)\n",
    "        self.ffn = PositionwiseFFN(d_ff=d_ff, d_model=d_model, dropout=dropout)\n",
    "        self.layernorms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape x: {x.shape}\")\n",
    "            print(f\"Encoder output shape: {enc_output.shape}\\n\")\n",
    "        # masked self-attention over the target (with look-ahead mask)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Passing through self-attention\")\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.layernorms[0](x + self.dropout(self_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nPassing Through encoder-decoder attention\")\n",
    "        # encoder-decoder attention over the encoder output (attend to source)\n",
    "        enc_dec_attn_output, _ = self.src_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.layernorms[1](x + self.dropout(enc_dec_attn_output))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal feedforward of layer\")\n",
    "        # feedforward with residual connection and layer normalization\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.layernorms[2](x + self.dropout(ffn_output))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nOutput shape: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_length):\n",
    "    \"\"\"\n",
    "    Creates a causal mask (look-ahead mask) that prevents attending to future tokens.\n",
    "    size: Length of the sequence.\n",
    "    \"\"\"\n",
    "    return torch.tril(torch.ones(seq_length, seq_length)).type(torch.uint8)  # Shape: (seq_length, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Num heads: 2\n",
      "Embedding dimension: 8\n",
      "per head dimension: 4\n",
      "Input shape x: torch.Size([1, 4, 8])\n",
      "Encoder output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 4, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([1, 1, 1, 1, 4])\n",
      "Query shape: torch.Size([1, 4, 8])\n",
      "Key shape: torch.Size([1, 4, 8])\n",
      "Value shape: torch.Size([1, 4, 8])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4]), torch.Size([1, 2, 4, 4])\n",
      "Scores shape: torch.Size([1, 2, 4, 4])\n",
      "Attention weights shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape: torch.Size([1, 1, 2, 4, 4])\n",
      "Attention output shape after concat: torch.Size([1, 4, 8])\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([1, 4, 8])\n",
      "Output from decoder layer: tensor([[[-0.2596, -2.1058,  0.3552,  0.2740,  1.1427, -0.8167,  0.2993,\n",
      "           1.1110],\n",
      "         [ 0.7586, -1.9316,  0.0183, -0.1806,  1.2117, -0.2289, -0.8431,\n",
      "           1.1954],\n",
      "         [-0.3422, -2.0611, -0.2723,  1.4369, -0.4463,  0.2744,  1.0418,\n",
      "           0.3689],\n",
      "         [ 0.4622, -1.4476, -0.5858,  0.6427,  0.7897, -1.3347, -0.1128,\n",
      "           1.5864]]], device='mps:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Random input sequence for target (decoder input)\n",
    "tgt = torch.rand(1, 4, 8).to(device)  # (batch_size=1, seq_length=4, d_model=8)\n",
    "\n",
    "# Random encoder output (assuming same dimensions for simplicity)\n",
    "enc_output = torch.rand(1, 4, 8).to(device)\n",
    "\n",
    "# Create masks\n",
    "tgt_mask = create_causal_mask(seq_length=4).unsqueeze(0).to(device)  # Causal mask for target\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]])).to(device)  # Padding mask for source\n",
    "\n",
    "# Initialize the decoder layer\n",
    "decoder_layer = DecoderLayer(num_heads=2, d_model=8, d_ff=16, dropout=0.1, verbose=True).to(device)\n",
    "\n",
    "# Pass through the decoder layer\n",
    "output = decoder_layer(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "print(\"Output from decoder layer:\", output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # encoder layers\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # final layer normalization layer\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask = None):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input of shape: {x.shape}\")\n",
    "        \n",
    "        for i, block in enumerate(self.encoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------ Passing Through Encoder block {i + 1} ----------------\")\n",
    "            \n",
    "            x = block(x, mask=src_mask)\n",
    "\n",
    "        # apply final layer normalization\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nFinal output shape is: {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoder.__init__() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m create_padding_mask(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m]]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the encoder with 2 blocks for testing\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Pass through the encoder\u001b[39;00m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m encoder(src, src_mask\u001b[38;5;241m=\u001b[39msrc_mask)\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.__init__() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "# Random input sequence (batch_size=1, seq_length=4, d_model=8)\n",
    "src = torch.rand(1, 4, 8)\n",
    "\n",
    "# Create a padding mask for the source sequence\n",
    "src_mask = create_padding_mask(torch.tensor([[1, 2, 3, 0]]))\n",
    "\n",
    "# Initialize the encoder with 2 blocks for testing\n",
    "encoder = Encoder(num_blocks=2, num_heads=2, d_model=8, d_ff=16, dropout=0.1, max_length=30, verbose=True)\n",
    "\n",
    "# Pass through the encoder\n",
    "output = encoder(src, src_mask=src_mask)\n",
    "print(\"Final output from encoder:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_mask', 'tgt_mask'])\n",
      "Source tokens: torch.Size([4, 30])\n",
      "tgt_input: torch.Size([4, 29])\n",
      "tgt_output: torch.Size([4, 29])\n",
      "src_mask: torch.Size([4, 1, 30])\n",
      "tgt_mask: torch.Size([4, 29, 29])\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "\n",
      "Source token shape: torch.Size([4, 30])\n",
      "Input of shape: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 1 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 2 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 3 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 4 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 5 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 6 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "Final output shape is: torch.Size([4, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "# test with actual examples\n",
    "batch_size = 4\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(batch.keys())\n",
    "    src_tokens = batch['src_tokens'].to(device)  # The tokenized source sentences\n",
    "    tgt_input = batch['tgt_input'].to(device)  # The tokenized target sentences\n",
    "    tgt_output = batch['tgt_output'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "    tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "    print(f\"Source tokens: {src_tokens.shape}\")\n",
    "    print(f\"tgt_input: {tgt_input.shape}\")\n",
    "    print(f\"tgt_output: {tgt_output.shape}\")\n",
    "    print(f\"src_mask: {src_mask.shape}\")\n",
    "    print(f\"tgt_mask: {tgt_mask.shape}\")\n",
    "\n",
    "    break  # Just getting the first batch for demonstration\n",
    "\n",
    "encoder = Encoder(num_blocks=6, num_heads=8, d_model=512, d_ff=2048, verbose=True).to(device)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), 512).to(device)\n",
    "pos_encoder = PositionalEncoding(512, dropout=0.1, max_len=512).to(device)\n",
    "\n",
    "for batch in small_dl:\n",
    "    src_tokens = batch['src_tokens'].to(device)\n",
    "    src_mask = batch['src_mask'].to(device)\n",
    "\n",
    "    print(f\"\\nSource token shape: {src_tokens.shape}\")\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_blocks: int, num_heads: int, d_model: int, d_ff: int, dropout: float = 0.1, verbose: bool = True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=verbose) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n------------- Passing Through Decoder Block {i+1} ----------------\")\n",
    "            tgt = block(tgt, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.layernorm(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5])\n",
      "torch.Size([4, 5, 5])\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "\n",
      "------------- Passing Through Decoder Block 1 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 2 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 3 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 4 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 5 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 6 ----------------\n",
      "Input shape x: torch.Size([4, 5, 512])\n",
      "Encoder output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 5, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 5])\n",
      "Query shape: torch.Size([4, 5, 512])\n",
      "Key shape: torch.Size([4, 5, 512])\n",
      "Value shape: torch.Size([4, 5, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64]), torch.Size([4, 8, 5, 64])\n",
      "Scores shape: torch.Size([4, 8, 5, 5])\n",
      "Attention weights shape: torch.Size([4, 8, 5, 5])\n",
      "Attention output shape: torch.Size([4, 8, 5, 64])\n",
      "Attention output shape after concat: torch.Size([4, 5, 512])\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 5, 512])\n",
      "Decoder output shape: torch.Size([4, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Mock data for testing\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "d_ff = 2048\n",
    "\n",
    "# Random embedded target tokens (already embedded, just mock data)\n",
    "tgt_embed = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Random encoder output (to simulate the output from the encoder)\n",
    "enc_output = torch.rand(batch_size, seq_length, d_model)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "# Create padding mask (mock data, assume no padding tokens for simplicity)\n",
    "src_mask = torch.ones(batch_size, 1, seq_length)  # Shape: (batch_size, 1, seq_length)\n",
    "print(src_mask.shape)\n",
    "\n",
    "tgt_mask = torch.tril(torch.ones(batch_size, seq_length, seq_length))\n",
    "print(tgt_mask.shape)\n",
    "\n",
    "\n",
    "# Initialize the decoder without embedding\n",
    "decoder = Decoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=0.1, verbose=True)\n",
    "\n",
    "# Pass the mock data through the decoder\n",
    "decoder_output = decoder(tgt_embed, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"Decoder output shape:\", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to instantiate an encoder and decoder class and string them together to confirm that everything works together. Than we will abstract and create an Encoder-Decoder sequence to sequence model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define the linear + softmax step for generating token probabilities.\n",
    "        Layer projects vector on to vocab space and then applys a log_softmax. \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Num heads: 8\n",
      "Embedding dimension: 512\n",
      "per head dimension: 64\n",
      "Source tokens: torch.Size([4, 30])\n",
      "Target input tokens: torch.Size([4, 29])\n",
      "Target output tokens: torch.Size([4, 29])\n",
      "Source mask: torch.Size([4, 1, 30])\n",
      "Target mask: torch.Size([4, 29, 29])\n",
      "Input of shape: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 1 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 2 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 3 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 4 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 5 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "------------ Passing Through Encoder block 6 ----------------\n",
      "Input to Encoder Layer: torch.Size([4, 30, 512])\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 30, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 30, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 30, 30])\n",
      "Attention output shape: torch.Size([4, 8, 30, 64])\n",
      "Attention output shape after concat: torch.Size([4, 30, 512])\n",
      "Output shape: torch.Size([4, 30, 512])\n",
      "attn_output shape: torch.Size([4, 30, 512])\n",
      "Output from Encoder Layer: torch.Size([4, 30, 512])\n",
      "\n",
      "Final output shape is: torch.Size([4, 30, 512])\n",
      "Encoder output: torch.Size([4, 30, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 1 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 2 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 3 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 4 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 5 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "------------- Passing Through Decoder Block 6 ----------------\n",
      "Input shape x: torch.Size([4, 29, 512])\n",
      "Encoder output shape: torch.Size([4, 30, 512])\n",
      "\n",
      "Passing through self-attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 29, 29])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 29, 512])\n",
      "Value shape: torch.Size([4, 29, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 29, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 29])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 29])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Passing Through encoder-decoder attention\n",
      "Mask shape (after unsqueezing at 1): torch.Size([4, 1, 1, 30])\n",
      "Query shape: torch.Size([4, 29, 512])\n",
      "Key shape: torch.Size([4, 30, 512])\n",
      "Value shape: torch.Size([4, 30, 512])\n",
      "Shapes after projections for query, key, value...\n",
      "torch.Size([4, 8, 29, 64]), torch.Size([4, 8, 30, 64]), torch.Size([4, 8, 30, 64])\n",
      "Scores shape: torch.Size([4, 8, 29, 30])\n",
      "Attention weights shape: torch.Size([4, 8, 29, 30])\n",
      "Attention output shape: torch.Size([4, 8, 29, 64])\n",
      "Attention output shape after concat: torch.Size([4, 29, 512])\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "\n",
      "Final feedforward of layer\n",
      "\n",
      "Output shape: torch.Size([4, 29, 512])\n",
      "tensor([[[ -9.5428, -11.4889, -11.3551,  ..., -10.5585, -11.9238, -11.5818],\n",
      "         [-10.5360, -12.6945, -12.3860,  ..., -11.5071, -11.1959, -11.0985],\n",
      "         [ -9.4768, -11.4089, -11.2051,  ..., -11.0061, -10.5254, -11.6813],\n",
      "         ...,\n",
      "         [-10.2705, -12.5026, -11.8268,  ..., -11.2842, -11.0900, -11.2056],\n",
      "         [-10.2119, -12.2296, -11.9960,  ..., -11.4058, -10.8501, -11.8093],\n",
      "         [-10.0226, -12.2076, -11.7713,  ..., -11.0998, -11.1339, -12.0274]],\n",
      "\n",
      "        [[-10.4049, -11.1491, -10.8724,  ..., -11.2115, -10.9812, -12.2988],\n",
      "         [-11.0263, -12.2623, -11.3992,  ..., -11.2973, -11.4610, -10.9185],\n",
      "         [-10.3469, -11.7898, -10.6566,  ..., -11.0866, -11.2022, -11.2241],\n",
      "         ...,\n",
      "         [-10.5694, -12.2760, -11.7436,  ..., -10.8654, -11.2371, -11.8884],\n",
      "         [-10.3513, -11.8226, -11.4644,  ..., -10.7998, -10.9644, -12.2493],\n",
      "         [-10.4903, -12.3309, -11.7619,  ..., -10.9208, -11.5885, -11.5313]],\n",
      "\n",
      "        [[-10.3872, -11.6403, -11.1660,  ..., -10.3757, -11.1475, -12.4671],\n",
      "         [-11.0921, -11.4703, -10.3661,  ..., -11.5168, -11.9217, -12.0199],\n",
      "         [-10.5593, -11.5442, -11.1277,  ..., -10.6850, -11.4532, -12.4511],\n",
      "         ...,\n",
      "         [-10.1421, -11.9919, -11.3924,  ..., -11.0073, -11.1904, -11.9340],\n",
      "         [ -9.8274, -11.8371, -11.5141,  ..., -10.7357, -11.0390, -11.9767],\n",
      "         [-10.1019, -11.9684, -11.1436,  ..., -10.6224, -10.9589, -12.0173]],\n",
      "\n",
      "        [[ -9.7988, -12.2293, -11.3498,  ..., -11.4351, -11.4059, -11.9511],\n",
      "         [-10.8743, -11.4673, -10.9902,  ..., -11.3318, -10.6566, -11.9442],\n",
      "         [-10.9878, -11.6559, -10.0992,  ..., -10.6680, -12.1806, -11.3811],\n",
      "         ...,\n",
      "         [-10.6903, -10.8997, -10.6066,  ..., -11.0893, -11.2124, -11.3291],\n",
      "         [-10.3294, -11.6776, -10.9276,  ..., -11.2956, -11.4628, -11.2253],\n",
      "         [-10.9522, -10.8514, -10.6157,  ..., -10.7077, -11.3863, -12.1285]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[ 5606,   992,    16,   385,   264,  1841,   304,  1459,    18,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [14422,    16,   942, 29696,    16,   352,  3404,   293,  5499,   304,\n",
      "           280,  4781,   538,   520,  1221, 14311,   795,   324,  8741,  4790,\n",
      "            18,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 4837,  8007,    16,  1694,    16,   372,   484, 10297,    11,   274,\n",
      "         20179,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 4342,   304,   280,  1120, 30049, 32639,   920,  5518,   286, 20043,\n",
      "         24555,   560,   942,   361,   355,   285,   335,  4630,   349,  1535,\n",
      "           349,    16,   925,  1391, 13058,   280,   670,  1140,  1221]])\n",
      "torch.Size([4, 29, 37000])\n",
      "torch.Size([4, 29])\n",
      "torch.Size([4, 29])\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 30\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "small_dl = DataLoader(small_translation_ds, batch_size = batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len)\n",
    "\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=True)\n",
    "\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size())\n",
    "\n",
    "for batch in small_dl:\n",
    "    print(f\"Source tokens:\", batch['src_tokens'].shape)\n",
    "    print(f\"Target input tokens:\", batch['tgt_input'].shape)\n",
    "    print(f\"Target output tokens:\", batch['tgt_output'].shape)\n",
    "    print(f\"Source mask:\", batch['src_mask'].shape)\n",
    "    print(f\"Target mask:\", batch['tgt_mask'].shape)\n",
    "\n",
    "    src_tokens = batch['src_tokens']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_input = batch['tgt_input']\n",
    "    tgt_output = batch['tgt_output']\n",
    "    src_mask = batch['src_mask']\n",
    "    tgt_mask = batch['tgt_mask']\n",
    "\n",
    "    src_embed = embedding(src_tokens)\n",
    "    src_embed = pos_encoder(src_embed)\n",
    "    encoder_output = encoder(src_embed, src_mask)\n",
    "\n",
    "    print(f\"Encoder output: {encoder_output.shape}\")\n",
    "\n",
    "    tgt_embed = embedding(tgt_input)\n",
    "    tgt_embed = pos_encoder(tgt_embed)\n",
    "    dec_output = decoder(tgt=tgt_embed, enc_output = encoder_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "    output = generator(dec_output)\n",
    "    predicted_tokens = torch.argmax(output, dim=-1)\n",
    "\n",
    "    print(output)\n",
    "    print(tgt_output)\n",
    "\n",
    "    print(output.shape)\n",
    "    print(predicted_tokens.shape)\n",
    "    print(tgt_output.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we abstract the above into a EncoderDecoder class. \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, generator: Generator, embedding: nn.Embedding, pos_encoder: PositionalEncoding, verbose: bool = False):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "        self.embedding = embedding\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, src_tokens, tgt_input, src_mask, tgt_mask):\n",
    "        # Encoder\n",
    "        src_embed = self.embedding(src_tokens)\n",
    "        src_embed = self.pos_encoder(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        tgt_embed = self.embedding(tgt_input)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "        dec_output = self.decoder(tgt=tgt_embed, enc_output=encoder_output, src_mask=src_mask, tgt_mask = tgt_mask)\n",
    "\n",
    "        output_log_probs = self.generator(dec_output)\n",
    "\n",
    "        return output_log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_ds = TranslationDataset(dataset['train'].shuffle().select(range(20000)), tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)\n",
    "val_ds = TranslationDataset(dataset['validation'], tokenizer=tokenizer, bos_token_id=BOS_TOKEN_ID, eos_token_id=EOS_TOKEN_ID, pad_token_id=PAD_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['src_tokens', 'tgt_input', 'tgt_output', 'src_mask', 'tgt_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate models\n",
    "\n",
    "# parameters\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "max_len = 512\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "embedding = nn.Embedding(tokenizer.get_vocab_size(), d_model).to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_len).to(device)\n",
    "encoder = Encoder(num_blocks=num_blocks, num_heads=num_heads, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "decoder = Decoder(num_blocks=6, num_heads=8, d_model=d_model, d_ff=d_ff, dropout=dropout, verbose=False).to(device)\n",
    "generator = Generator(d_model=d_model, vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder, generator, embedding, pos_encoder, verbose=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate scheduler, following `Attention is All You Need` for now. \n",
    "# lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n",
    "warmup_steps = 4000\n",
    "\n",
    "def get_lr(step_num):\n",
    "    return d_model ** -0.5 * min(step_num ** -0.5, step_num * warmup_steps ** -1.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c9e48a85544f0c8ad61a53666116a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average training loss:  0.6738\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f6f79e9bb43fba350a06e0b27eaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Average validation loss: 0.3290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf9b7e30b48485aaebc9d418c12cdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average training loss:  0.3582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa4de02c60448e990bf3ac17c211ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Average validation loss: 0.3121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0075c422264a739fa926c9990fa431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average training loss:  0.3347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597fefd8b990430c882736d925d33af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Average validation loss: 0.3031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cbb86a817f46218fff1d04f3563fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average training loss:  0.3166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7622a96fd499c98ed9b8e2d534e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Average validation loss: 0.2938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790f1b800f454cbd9aa6290ce2ac8aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average training loss:  0.3002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ab48c9bf7e4709a031af3f966b7040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Average validation loss: 0.2897\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# training loop\n",
    "# optimizer and criterion\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step_num = 0\n",
    "\n",
    "    for batch in tqdm(train_dl):\n",
    "        step_num += 1\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = get_lr(step_num)\n",
    "\n",
    "        src_tokens = batch['src_tokens'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        src_mask = batch['src_mask'].to(device)\n",
    "        tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "        # print(src_tokens.device)\n",
    "        # print(src_mask.device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_logits = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = criterion(output_logits.view(-1, output_logits.size(-1)), tgt_output.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average training loss: {avg_loss: .4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl):\n",
    "            src_tokens = batch['src_tokens'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_output = batch['tgt_output'].to(device)\n",
    "            src_mask = batch['src_mask'].to(device)\n",
    "            tgt_mask = batch['tgt_mask'].to(device)\n",
    "\n",
    "            output = model(src_tokens, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dl)\n",
    "    print(F\"Epoch {epoch + 1}/{num_epochs}, Average validation loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src_sentence': 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten',\n",
       "  'tgt_sentence': 'A Republican strategy to counter the re-election of Obama',\n",
       "  'src_tokens': tensor([    0,  2530,  2878, 12244,  8708,  4789,    16,   577,   319,  4755,\n",
       "           3815,   408, 11741,  7738, 27237,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,    37,  5929,   279,  4024,   313,  7273,   280,   351,    17,\n",
       "           4357,   675,   304, 11741,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.',\n",
       "  'tgt_sentence': 'Republican leaders justified their policy by the need to combat electoral fraud.',\n",
       "  'src_tokens': tensor([    0,   567, 29354,  6387,   319, 30529, 12897, 20929,  1090,  2346,\n",
       "            437,   319,  5622,    16,   389,  3041,  3502, 27218,   386, 12398,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0, 25904,  1312,   279,  5465, 14716,   808,  1458,   519,   280,\n",
       "            943,   313,  8810, 18700, 10601,    18,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.',\n",
       "  'tgt_sentence': 'However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.',\n",
       "  'src_tokens': tensor([    0,  8067,  9096,   384, 13181,   279,  7277, 13557,   268,   430,\n",
       "            719, 25411,   394,    16,  3623,   531, 21506,    16,   521,   319,\n",
       "           3041,  3502, 27218,   286,   389,  2821, 10610,   262,   423,   475,\n",
       "            317,  5697,   319,  1194, 35674,  4968,  1423,   365,  9984,  1228,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  2395,    16,   280, 13181,   279,  6785, 16453,   484,   264,\n",
       "          19725,    16, 24799,   393, 18700, 10601,   326,   409,   285,   262,\n",
       "            286,   280,  2713,  1162,  1189,   280,  1679,   304,  1120, 14608,\n",
       "            519,  3772,  4097,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Die Rechtsanwälte der Republikaner haben in 10 Jahren in den USA übrigens nur 300 Fälle von Wahlbetrug verzeichnet.',\n",
       "  'tgt_sentence': 'Indeed, Republican lawyers identified only 300 cases of electoral fraud in the United States in a decade.',\n",
       "  'src_tokens': tensor([    0,   567, 31236, 19176,   319, 30529, 12897,   672,   286,  1523,\n",
       "           1802,   286,   389,  2821, 13316,   848,  6139, 11221,   408,  3041,\n",
       "           3502, 27218, 34763,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  8342,    16,  5929,   279, 23670, 13771,   962,  6139,  4545,\n",
       "            304, 18700, 10601,   286,   280,  2713,  1162,   286,   264, 15815,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Eins ist sicher: diese neuen Bestimmungen werden sich negativ auf die Wahlbeteiligung auswirken.',\n",
       "  'tgt_sentence': 'One thing is certain: these new provisions will have a negative impact on voter turn-out.',\n",
       "  'src_tokens': tensor([    0,  1568,    87,   423,  2276,    30,   843,  1777,  6256,   514,\n",
       "            508, 20577,   428,   317,  3041, 29507, 18584,    18,     2,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([   0, 4342, 4839,  326, 2072,   30, 1032,  874, 6454,  541,  520,  264,\n",
       "          7650, 4926,  385, 2493,  338, 3173,   17,  688,   18,    2,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "             1,    1,    1,    1,    1,    1,    1,    1])},\n",
       " {'src_sentence': 'In diesem Sinne untergraben diese Maßnahmen teilweise das demokratische System der USA.',\n",
       "  'tgt_sentence': 'In this sense, the measures will partially undermine the American democratic system.',\n",
       "  'src_tokens': tensor([    0,   588,  1060,  6632, 23110,   843,  2092,  8778,   384, 11022,\n",
       "           2705,   319,  2821,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   588,   484,  5477,    16,   280,  2372,   541, 23351, 20676,\n",
       "            280,  4984,  4696,  1419,    18,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Im Gegensatz zu Kanada sind die US-Bundesstaaten für die Durchführung der Wahlen in den einzelnen Staaten verantwortlich.',\n",
       "  'tgt_sentence': 'Unlike in Canada, the American States are responsible for the organisation of federal elections in the United States.',\n",
       "  'src_tokens': tensor([    0,  1501, 10183,   386, 15817,   580,   317,  1377,    17, 33664,\n",
       "           1296,   430,   317,  8023,   319,  8005,   286,   389,  4851,  3122,\n",
       "           7388,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0, 25618,   286, 12688,    16,   280,  4984,  1162,   439,  4276,\n",
       "            372,   280,  8848,   304, 14593,  6752,   286,   280,  2713,  1162,\n",
       "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'In diesem Sinne hat die Mehrheit der amerikanischen Regierungen seit 2009 neue Gesetze verkündet, die das Verfahren für die Registrierung oder den Urnengang erschweren.',\n",
       "  'tgt_sentence': 'It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.',\n",
       "  'src_tokens': tensor([    0,   588,  1060,  6632,   647,   317,  5866,   319,  9309,  5915,\n",
       "           2136,  2245,  1663, 11424, 36981,    16,   317,   384,  4528,   430,\n",
       "            317, 15918,   653,   389,  2435,   472,  2487, 33852,    18,     2,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   897,   326,   286,   484,  6897,   393,   264,  4871,   304,\n",
       "           4984,  5601,   520,  9975,   874,  7871,  2255,  2245,  3274,   280,\n",
       "          10713,   494,  7287,  1539,   801,  2855,    18,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Dieses Phänomen hat nach den Wahlen vom November 2010 an Bedeutung gewonnen, bei denen 675 neue republikanische Vertreter in 26 Staaten verzeichnet werden konnten.',\n",
       "  'tgt_sentence': 'This phenomenon gained momentum following the November 2010 elections, which saw 675 new Republican representatives added in 26 States.',\n",
       "  'src_tokens': tensor([    0,  3890, 19491,   647,   715,   389,  8005,  1194,  4334,  2789,\n",
       "            293,  2819, 12400,    16,   690,  1856,  1322,  9528,  1663,  2878,\n",
       "          12244,  8708,  7001,   286,  6225,  3122, 34763,   514,  6518,    18,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,   887, 16175, 14218, 28840,  3301,   280,  4334,  2789,  6752,\n",
       "             16,   557, 10104,  1322,  9528,   874,  5929,   279,  7598,  5781,\n",
       "            286,  6225,  1162,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])},\n",
       " {'src_sentence': 'Infolgedessen wurden 180 Gesetzesentwürfe allein im Jahr 2011 eingeführt, die die Ausübung des Wahlrechts in 41 Staaten einschränken.',\n",
       "  'tgt_sentence': 'As a result, 180 bills restricting the exercise of the right to vote in 41 States were introduced in 2011 alone.',\n",
       "  'src_tokens': tensor([    0,   588, 28261,  1527, 18189,  4219,  1206, 27824,  6223,   414,\n",
       "            904,  9274,  9657,    16,   317,   317, 19744,   434,  3041,  4739,\n",
       "            286, 18439,  3122, 30130,    18,     2,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]),\n",
       "  'tgt_tokens': tensor([    0,  1683,   264,  2404,    16, 18189, 36515, 35318,   280,  9761,\n",
       "            304,   280,  1475,   313,  2675,   286, 18439,  1162,  1121,  8094,\n",
       "            286,  9274,  7757,    18,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1])}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if inference works\n",
    "num_examples = 10\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(val_ds[i])\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(37000, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(model: EncoderDecoder, src_tokens: torch.Tensor, tokenizer: ByteLevelBPETokenizer = tokenizer, pad_token_id: int = PAD_TOKEN_ID, max_len=100):\n",
    "    model.eval()\n",
    "\n",
    "    # embed the source tokens and create the src_mask\n",
    "    src_tokens = src_tokens.unsqueeze(0).to(device)\n",
    "    src_mask = (src_tokens != pad_token_id).unsqueeze(0).to(device) # shape: (1, seq_length)\n",
    "\n",
    "    src_embed = model.embedding(src_tokens)\n",
    "    src_embed = model.pos_encoder(src_embed)\n",
    "\n",
    "    # store encoder hidden states for the src_tokens\n",
    "    encoder_output = model.encoder(src_embed, src_mask)\n",
    "\n",
    "    # initizlie target sentence with BOS token\n",
    "    tgt_tokens = torch.tensor([BOS_TOKEN_ID], dtype=torch.long).to(device)\n",
    "\n",
    "    # Autoregressive loop to generate sentence\n",
    "    for _ in range(max_len):\n",
    "        # create target mask\n",
    "        tgt_seq_len = tgt_tokens.size(0)\n",
    "        tgt_mask = torch.tril(torch.ones(1, tgt_seq_len, tgt_seq_len)).to(device)\n",
    "        #print(f\"target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "        #print(f\"Tokens at beginning: {tgt_tokens}\")\n",
    "        tgt_embed = model.embedding(tgt_tokens).unsqueeze(0)\n",
    "        #print(f\"Token embeddings shape: {tgt_embed.shape}\")\n",
    "        tgt_embed = model.pos_encoder(tgt_embed)\n",
    "\n",
    "        output_logits = model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "        output_log_probs = model.generator(output_logits)\n",
    "        # print(output_log_probs.shape)\n",
    "        # print(output_log_probs)\n",
    "        next_token = torch.argmax(output_log_probs[:, -1, :], dim=-1)\n",
    "        # print(next_token.shape)\n",
    "        #print(f\"Next token: {next_token.item()}\")\n",
    "        #print(tgt_tokens.shape)\n",
    "        # append next\n",
    "        tgt_tokens = torch.cat([tgt_tokens, next_token])\n",
    "\n",
    "        if next_token.item() == EOS_TOKEN_ID or tgt_tokens.size(0) >= 50:\n",
    "            break\n",
    "\n",
    "        #print(f\"Resulting Target Tokens: {tgt_tokens}\")\n",
    "\n",
    "    print(f\"Source sentence: {tokenizer.decode([num for num in src_tokens.squeeze(0).tolist() if num != pad_token_id], skip_special_tokens=True)}\")\n",
    "    print(f\"Translation: {tokenizer.decode(tgt_tokens.tolist())}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sampling(logits, k=10):\n",
    "    values, indices = torch.topk(logits, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(src_tokens.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.id_to_token(BOS_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: <s>Die Konservierung von Fleisch durch Räuchern, Trocknen oder Salzen kann zur Bildung von Karzinogenen führen.</s>\n",
      "Translation: <s>The town of the town of the town can be used to the same time, or to the use of the other hand.</s>\n",
      "Source sentence: <s>Deshalb empfehle ich den Test ab einem Alter von 50 Jahren bzw. 40 Jahren, wenn man einen direkten Verwandten hat, der bereits an Prostatakrebs erkrankt war.</s>\n",
      "Translation: <s>I therefore believe that the fact that a few years of the years of the years, if the last two years have been made a few years of the last year.</s>\n",
      "Source sentence: <s>Man schlägt ihnen eine aktive Überwachung vor und bietet ihnen bei Fortschreiten der Krankheit eine Behandlung an.</s>\n",
      "Translation: <s>You can enjoy a high-quality and a long-term and a long-term way.</s>\n",
      "Source sentence: <s>\"Die Durchführung eines Früherkennungstest führt nicht zu Krebs.\"</s>\n",
      "Translation: <s>The fact is to make a few words of a few words of the most important issues.</s>\n",
      "Source sentence: <s>70% Schwarzgeld in den Wahlkampagnen</s>\n",
      "Translation: <s>The% of the United Kingdom in the United Kingdom.</s>\n",
      "Source sentence: <s>Beispielsweise haben nur 16 der 34 Bundesstaaten Gesetze verabschiedet, die das Vorzeigen eines Lichtbildausweises verlangen.</s>\n",
      "Translation: <s>The only way of the most important points out of the most important aspects of a few days.</s>\n",
      "Source sentence: <s>Natürlich haben sich die demokratischen Gesetzgeber und ihre Anhänger energisch der Verabschiedung von Gesetzen entgegengestellt, die die Registrierung der Wähler einschränken.</s>\n",
      "Translation: <s>The situation has taken by the Committee on Legal Affairs and the Committee on the Lisbon Strategy, which is the right of the Community.</s>\n",
      "Source sentence: <s>In der letzten Saison waren wir dicht dran.</s>\n",
      "Translation: <s>In the last two years, we have been made in the last year.</s>\n",
      "Source sentence: <s>Sowohl auf Ebene des körperlichen, emotionellen oder spirituellen Komforts.</s>\n",
      "Translation: <s>The Council of the Committee on Budgets, or sub-based services.</s>\n",
      "Source sentence: <s>Wir haben mit 79 Punkten abgeschlossen.</s>\n",
      "Translation: <s>We have a great deal with the same way.</s>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_integers = np.random.choice(300, 10, replace=False).tolist()\n",
    "examples = [val_ds[i]['src_tokens'] for i in random_integers]\n",
    "for example in examples:\n",
    "    greedy_decoding(model, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = val_ds[170]['src_tokens'].unsqueeze(0).to(device)\n",
    "src_sentence = val_ds[170]['src_sentence']\n",
    "src_mask = (src_tokens != PAD_TOKEN_ID).unsqueeze(0).to(device)\n",
    "src_mask.shape, src_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93, 60, 66, 102, 238, 45, 41, 294, 178, 297]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embed = model.embedding(src_tokens)\n",
    "src_embed = model.pos_encoder(src_embed)\n",
    "encoder_output = model.encoder(src_embed, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target mask shape: torch.Size([1, 1, 1])\n",
      "Tokens at beginning: tensor([0], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 1, 512])\n",
      "None\n",
      "Next token: 45\n",
      "torch.Size([1])\n",
      "Resulting Target Tokens: tensor([ 0, 45], device='mps:0')\n",
      "target mask shape: torch.Size([1, 2, 2])\n",
      "Tokens at beginning: tensor([ 0, 45], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 2, 512])\n",
      "None\n",
      "Next token: 520\n",
      "torch.Size([2])\n",
      "Resulting Target Tokens: tensor([  0,  45, 520], device='mps:0')\n",
      "target mask shape: torch.Size([1, 3, 3])\n",
      "Tokens at beginning: tensor([  0,  45, 520], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 3, 512])\n",
      "None\n",
      "Next token: 956\n",
      "torch.Size([3])\n",
      "Resulting Target Tokens: tensor([  0,  45, 520, 956], device='mps:0')\n",
      "target mask shape: torch.Size([1, 4, 4])\n",
      "Tokens at beginning: tensor([  0,  45, 520, 956], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 4, 512])\n",
      "None\n",
      "Next token: 7014\n",
      "torch.Size([4])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014], device='mps:0')\n",
      "target mask shape: torch.Size([1, 5, 5])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 5, 512])\n",
      "None\n",
      "Next token: 393\n",
      "torch.Size([5])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393], device='mps:0')\n",
      "target mask shape: torch.Size([1, 6, 6])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 6, 512])\n",
      "None\n",
      "Next token: 435\n",
      "torch.Size([6])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435], device='mps:0')\n",
      "target mask shape: torch.Size([1, 7, 7])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 7, 512])\n",
      "None\n",
      "Next token: 326\n",
      "torch.Size([7])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326], device='mps:0')\n",
      "target mask shape: torch.Size([1, 8, 8])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 8, 512])\n",
      "None\n",
      "Next token: 503\n",
      "torch.Size([8])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503], device='mps:0')\n",
      "target mask shape: torch.Size([1, 9, 9])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 9, 512])\n",
      "None\n",
      "Next token: 264\n",
      "torch.Size([9])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 10, 10])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 10, 512])\n",
      "None\n",
      "Next token: 1353\n",
      "torch.Size([10])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 11, 11])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 11, 512])\n",
      "None\n",
      "Next token: 4839\n",
      "torch.Size([11])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 12, 12])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 12, 512])\n",
      "None\n",
      "Next token: 393\n",
      "torch.Size([12])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393], device='mps:0')\n",
      "target mask shape: torch.Size([1, 13, 13])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 13, 512])\n",
      "None\n",
      "Next token: 938\n",
      "torch.Size([13])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938], device='mps:0')\n",
      "target mask shape: torch.Size([1, 14, 14])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 14, 512])\n",
      "None\n",
      "Next token: 520\n",
      "torch.Size([14])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520], device='mps:0')\n",
      "target mask shape: torch.Size([1, 15, 15])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 15, 512])\n",
      "None\n",
      "Next token: 264\n",
      "torch.Size([15])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264], device='mps:0')\n",
      "target mask shape: torch.Size([1, 16, 16])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 16, 512])\n",
      "None\n",
      "Next token: 1362\n",
      "torch.Size([16])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362], device='mps:0')\n",
      "target mask shape: torch.Size([1, 17, 17])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 17, 512])\n",
      "None\n",
      "Next token: 3069\n",
      "torch.Size([17])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069], device='mps:0')\n",
      "target mask shape: torch.Size([1, 18, 18])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 18, 512])\n",
      "None\n",
      "Next token: 427\n",
      "torch.Size([18])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427], device='mps:0')\n",
      "target mask shape: torch.Size([1, 19, 19])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 19, 512])\n",
      "None\n",
      "Next token: 264\n",
      "torch.Size([19])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264], device='mps:0')\n",
      "target mask shape: torch.Size([1, 20, 20])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 20, 512])\n",
      "None\n",
      "Next token: 1362\n",
      "torch.Size([20])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362], device='mps:0')\n",
      "target mask shape: torch.Size([1, 21, 21])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 21, 512])\n",
      "None\n",
      "Next token: 3069\n",
      "torch.Size([21])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 22, 22])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 22, 512])\n",
      "None\n",
      "Next token: 304\n",
      "torch.Size([22])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 23, 23])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 23, 512])\n",
      "None\n",
      "Next token: 280\n",
      "torch.Size([23])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280],\n",
      "       device='mps:0')\n",
      "target mask shape: torch.Size([1, 24, 24])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280],\n",
      "       device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 24, 512])\n",
      "None\n",
      "Next token: 1641\n",
      "torch.Size([24])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641], device='mps:0')\n",
      "target mask shape: torch.Size([1, 25, 25])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 25, 512])\n",
      "None\n",
      "Next token: 978\n",
      "torch.Size([25])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641,  978], device='mps:0')\n",
      "target mask shape: torch.Size([1, 26, 26])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641,  978], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 26, 512])\n",
      "None\n",
      "Next token: 18\n",
      "torch.Size([26])\n",
      "Resulting Target Tokens: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641,  978,   18], device='mps:0')\n",
      "target mask shape: torch.Size([1, 27, 27])\n",
      "Tokens at beginning: tensor([   0,   45,  520,  956, 7014,  393,  435,  326,  503,  264, 1353, 4839,\n",
      "         393,  938,  520,  264, 1362, 3069,  427,  264, 1362, 3069,  304,  280,\n",
      "        1641,  978,   18], device='mps:0')\n",
      "Token embeddings shape: torch.Size([1, 27, 512])\n",
      "None\n",
      "Next token: 2\n",
      "torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "tgt_tokens = torch.tensor([BOS_TOKEN_ID], dtype=torch.long).to(device)\n",
    "# tgt_mask = torch.ones(1, 1).to(device)\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    # create target mask\n",
    "    tgt_seq_len = tgt_tokens.size(0)\n",
    "    tgt_mask = torch.tril(torch.ones(1, tgt_seq_len, tgt_seq_len)).to(device)\n",
    "    print(f\"target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "    print(f\"Tokens at beginning: {tgt_tokens}\")\n",
    "    tgt_embed = model.embedding(tgt_tokens).unsqueeze(0)\n",
    "    print(print(f\"Token embeddings shape: {tgt_embed.shape}\"))\n",
    "    tgt_embed = model.pos_encoder(tgt_embed)\n",
    "\n",
    "    output_logits = model.decoder(tgt_embed, encoder_output, src_mask, tgt_mask)\n",
    "    output_log_probs = model.generator(output_logits)\n",
    "    # print(output_log_probs.shape)\n",
    "    # print(output_log_probs)\n",
    "    next_token = torch.argmax(output_log_probs[:, -1, :], dim=-1)\n",
    "    # print(next_token.shape)\n",
    "    print(f\"Next token: {next_token.item()}\")\n",
    "    print(tgt_tokens.shape)\n",
    "    # append next\n",
    "    tgt_tokens = torch.cat([tgt_tokens, next_token])\n",
    "\n",
    "    if next_token.item() == EOS_TOKEN_ID or tgt_tokens.size(0) >= 50:\n",
    "        break\n",
    "\n",
    "    print(f\"Resulting Target Tokens: {tgt_tokens}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 45,\n",
       " 520,\n",
       " 956,\n",
       " 7014,\n",
       " 393,\n",
       " 435,\n",
       " 326,\n",
       " 503,\n",
       " 264,\n",
       " 1353,\n",
       " 4839,\n",
       " 393,\n",
       " 938,\n",
       " 520,\n",
       " 264,\n",
       " 1362,\n",
       " 3069,\n",
       " 427,\n",
       " 264,\n",
       " 1362,\n",
       " 3069,\n",
       " 304,\n",
       " 280,\n",
       " 1641,\n",
       " 978,\n",
       " 18,\n",
       " 2]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ich habe keine Wünsche mehr im Leben\", sagt sie, bevor sie akzeptiert, dass man ihr eine Maske aufsetzt, die ihr beim Atmen hilft.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>I have no doubt that it is not a good thing that they have a great deal with a great deal of the same time.</s>'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(src_sentence)\n",
    "tokenizer.decode(tgt_tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    0,   567, 18680,   875,   408, 13262,   745,   370,   314,   412,\n",
       "           547,    16, 28152,   472,   653,  4206,  1582,   836,   665,  6417,\n",
       "           408,  4518, 11877,  2683,   261,  2660,    18,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  3905, 28806,   649,   389,  7363,   543,   809, 11218,   408,\n",
       "          2817,  1802,  3579,    18,  3921,  1802,    16,   980,   637,   719,\n",
       "         12944, 29619,   342,   647,    16,   319,  1717,   293,   591, 11135,\n",
       "           478,   281,  1145,  5881,   563,   420,   786,    18,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  3843, 13778,  3110,   477, 14529,  9601,   595,   320,  1943,\n",
       "          3110,   690,  4109,  8560,   319, 14049,   477,  7062,   293,    18,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,     6,   567,  8023,  1175, 13137,   676, 35795,   322,   392,\n",
       "          4803,   497,   386, 18678, 16527,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  7573,     9, 15849, 20415,   286,   389,  3041, 36760,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0, 28945,   672,   848,  2719,   319, 13010,  6562,  1296, 11424,\n",
       "         12725,    16,   317,   384,   792,  7866,  1175,  8072,  3102,   549,\n",
       "           518,  2558, 11512,    18,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,  7521,   672,   508,   317,  8099, 25214,   320,  1090, 22539,\n",
       "         33113,   319, 19383,   408, 24768,  7738,  2805,    16,   317,   317,\n",
       "         15918,   319, 15055, 30130,    18,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,   588,   319,  2751, 14759,  2458,   419, 26602,   266,   563,\n",
       "            18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0, 19580,   428,  4090,   434, 18686,   682,    16, 14085,  4364,\n",
       "           653,  6897,  3258,  8557,    87,    18,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([    0,   886,   672,   437, 27963, 12424,  9465,    18,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between example 0 and example 1: 225.04786682128906\n",
      "Difference between example 0 and example 2: 214.9784393310547\n",
      "Difference between example 0 and example 3: 222.360107421875\n",
      "Difference between example 0 and example 4: 259.3667907714844\n",
      "Difference between example 0 and example 5: 191.9576416015625\n",
      "Difference between example 0 and example 6: 214.24774169921875\n",
      "Difference between example 0 and example 7: 224.41213989257812\n",
      "Difference between example 0 and example 8: 185.67291259765625\n",
      "Difference between example 0 and example 9: 227.0283966064453\n",
      "Difference between example 1 and example 2: 251.41867065429688\n",
      "Difference between example 1 and example 3: 259.7427062988281\n",
      "Difference between example 1 and example 4: 282.7121276855469\n",
      "Difference between example 1 and example 5: 205.73074340820312\n",
      "Difference between example 1 and example 6: 205.10382080078125\n",
      "Difference between example 1 and example 7: 259.2537841796875\n",
      "Difference between example 1 and example 8: 231.93775939941406\n",
      "Difference between example 1 and example 9: 265.6619567871094\n",
      "Difference between example 2 and example 3: 224.1000518798828\n",
      "Difference between example 2 and example 4: 256.07257080078125\n",
      "Difference between example 2 and example 5: 219.65267944335938\n",
      "Difference between example 2 and example 6: 235.7158660888672\n",
      "Difference between example 2 and example 7: 206.96348571777344\n",
      "Difference between example 2 and example 8: 212.021484375\n",
      "Difference between example 2 and example 9: 219.85244750976562\n",
      "Difference between example 3 and example 4: 194.06747436523438\n",
      "Difference between example 3 and example 5: 220.6757354736328\n",
      "Difference between example 3 and example 6: 240.03964233398438\n",
      "Difference between example 3 and example 7: 210.8081512451172\n",
      "Difference between example 3 and example 8: 208.61041259765625\n",
      "Difference between example 3 and example 9: 202.48252868652344\n",
      "Difference between example 4 and example 5: 250.25869750976562\n",
      "Difference between example 4 and example 6: 265.3085021972656\n",
      "Difference between example 4 and example 7: 218.71737670898438\n",
      "Difference between example 4 and example 8: 241.65467834472656\n",
      "Difference between example 4 and example 9: 203.34730529785156\n",
      "Difference between example 5 and example 6: 176.19058227539062\n",
      "Difference between example 5 and example 7: 219.0852813720703\n",
      "Difference between example 5 and example 8: 191.37120056152344\n",
      "Difference between example 5 and example 9: 222.62713623046875\n",
      "Difference between example 6 and example 7: 237.59616088867188\n",
      "Difference between example 6 and example 8: 205.0058135986328\n",
      "Difference between example 6 and example 9: 238.35806274414062\n",
      "Difference between example 7 and example 8: 208.1018829345703\n",
      "Difference between example 7 and example 9: 172.05731201171875\n",
      "Difference between example 8 and example 9: 200.00473022460938\n"
     ]
    }
   ],
   "source": [
    "# collect encodings for the src sentences\n",
    "encodings = []\n",
    "for src_tokens in examples:\n",
    "    src_tokens = src_tokens.unsqueeze(0).to(device)\n",
    "    src_mask = (src_tokens != PAD_TOKEN_ID).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_embed = model.embedding(src_tokens)\n",
    "        src_embed = model.pos_encoder(src_embed)\n",
    "        encoder_output = model.encoder(src_embed, src_mask)\n",
    "\n",
    "    encodings.append(encoder_output)\n",
    "\n",
    "encoder_diffs = []\n",
    "for i in range(9):\n",
    "    for j in range(i+1, 10):\n",
    "        diff = torch.norm(encodings[i] - encodings[j])\n",
    "        encoder_diffs.append(diff)\n",
    "        print(f\"Difference between example {i} and example {j}: {diff}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(110.6548, device='mps:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(encoder_diffs) - min(encoder_diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
