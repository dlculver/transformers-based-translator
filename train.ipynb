{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import components\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WMT14 dataset for German-English translation\n",
    "dataset = load_dataset('wmt14', 'de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook, we will train on a small segment of the dataset as we will be working locally. \n",
    "# We will figure out the parameters and then train on the full set in the cloud. \n",
    "\n",
    "# Take a small subset for experimentation\n",
    "small_train_dataset = dataset['train'].select(range(10000))\n",
    "small_val_dataset = dataset['validation'].select(range(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are following the original `Attention is all you need paper` we will use Byte-Pair Encoding\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode',\n",
       "  'en': 'Resumption of the session'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4508785/4508785 [00:56<00:00, 80433.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# we will be training our own BPE tokenizer for this task. \n",
    "\n",
    "with open('train_texts.txt', 'w', encoding='utf-8') as f:\n",
    "\n",
    "    for example in tqdm(dataset['train']):\n",
    "        f.write(example['translation']['de'] + '\\n')\n",
    "        f.write(example['translation']['en'] + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train a BPE tokenizer\n",
    "bpe_tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer.train(\n",
    "    files=['train_texts.txt'],\n",
    "    vocab_size=37000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpe_tokenizer/vocab.json', 'bpe_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the tokenizer\n",
    "save_directory = 'bpe_tokenizer'\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "bpe_tokenizer.save_model('bpe_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"bpe_tokenizer/vocab.json\",\n",
    "    \"bpe_tokenizer/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[789, 423, 328, 3010, 18]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Das ist ein Beispiel.\").ids)\n",
    "# Should return something like ['<s>', 'Das', 'ist', 'ein', 'Beispiel', '</s>']\n",
    "\n",
    "print(tokenizer.token_to_id(\"</s>\"))\n",
    "# Should return a valid token ID for '</s>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "\n",
    "    # Extract German and English sentences from the list of dictionaries\n",
    "    src_texts = [example['de'] for example in examples['translation']]\n",
    "    tgt_texts = [example['en'] for example in examples['translation']]\n",
    "\n",
    "    # tokenize src and tgt\n",
    "    src_tokens = tokenizer.encode_batch(src_texts)\n",
    "    tgt_tokens = tokenizer.encode_batch(tgt_texts)\n",
    "\n",
    "    # return dictionary format expected by PyTorch\n",
    "    return {\n",
    "        'input_ids': [[tokenizer.token_to_id('<s>')] + encoding.ids + [tokenizer.token_to_id('</s>')] for encoding in src_tokens],\n",
    "        'attention_mask': [[tokenizer.token_to_id('<pad>')] + encoding.attention_mask + [tokenizer.token_to_id('<pad>')] for encoding in src_tokens],\n",
    "        'labels': [[tokenizer.token_to_id('<s>')] + encoding.ids + [tokenizer.token_to_id('</s>')] for encoding in tgt_tokens]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f51310c745d4e229e72d9343e86b107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the data\n",
    "tokenized_train = small_train_dataset.map(tokenize, batched=True)\n",
    "tokenized_val = small_val_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tokenized_train[98]\n",
    "\n",
    "assert len(example['input_ids']) == len(example['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode',\n",
       "  'en': 'Resumption of the session'},\n",
       " 'input_ids': [0, 23062, 17719, 319, 26699, 2],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1],\n",
       " 'labels': [0, 8859, 27958, 304, 280, 9974, 2]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Resumption of the session</s>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train[0]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to write a collate_fn to pad sentences to be of the same size\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    attention_mask = [torch.tensor(item['attention_mask']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.token_to_id('<pad>'))\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=tokenizer.token_to_id('<pad>'))\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# create the data loaders\n",
    "train_dl = DataLoader(\n",
    "    tokenized_train, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    tokenized_val, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([16, 92])\n",
      "Attention Mask: torch.Size([16, 92])\n",
      "Labels: torch.Size([16, 89])\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch from the training DataLoader\n",
    "for batch in train_dl:\n",
    "    print(\"Input IDs:\", batch['input_ids'].shape)\n",
    "    print(\"Attention Mask:\", batch['attention_mask'].shape)\n",
    "    print(\"Labels:\", batch['labels'].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def make_std_mask(tgt, pad):\n",
    "    \"Create a mask to hide padding and future words.\"\n",
    "    #print(\"Target (tgt):\", tgt)\n",
    "    \n",
    "    # Padding mask\n",
    "    tgt_padding_mask = (tgt != pad).unsqueeze(1).unsqueeze(2)\n",
    "    #print(\"Padding Mask:\", tgt_padding_mask)\n",
    "    \n",
    "    # Look-ahead mask (subsequent mask)\n",
    "    tgt_seq_len = tgt.size(-1)\n",
    "    look_ahead_mask = torch.triu(torch.ones((1, tgt_seq_len, tgt_seq_len), device=tgt.device), diagonal=1).type_as(tgt_padding_mask.data)\n",
    "    #print(\"Look-Ahead Mask (Subsequent Mask):\", look_ahead_mask)\n",
    "    \n",
    "    # Combined mask\n",
    "    tgt_mask = tgt_padding_mask & (look_ahead_mask == 0)\n",
    "    #print(\"Combined Target Mask:\", tgt_mask)\n",
    "    \n",
    "    return tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence IDs: [32, 87, 34, 465, 16218, 22524, 385, 280, 4226, 18, 32, 19, 87, 34]\n",
      "Padded Tokenized Sentence Tensor: tensor([[   32,    87,    34,   465, 16218, 22524,   385,   280,  4226,    18,\n",
      "            32,    19,    87,    34,     1,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "# Example English sentence\n",
    "sentence = \"<s>The cat sat on the mat.</s>\"\n",
    "\n",
    "# Tokenize the sentence using your trained tokenizer\n",
    "tgt_tokens = tokenizer.encode(sentence)\n",
    "tgt_token_ids = tgt_tokens.ids  # Get the list of token IDs\n",
    "print(\"Tokenized Sentence IDs:\", tgt_token_ids)\n",
    "\n",
    "# Convert the token IDs to a tensor (assuming <pad> token ID is 0)\n",
    "tgt_tensor = torch.tensor([tgt_token_ids + [tokenizer.token_to_id('<pad>')] * (20 - len(tgt_token_ids))])  # Pad to length 10\n",
    "print(\"Padded Tokenized Sentence Tensor:\", tgt_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           False, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True, False, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True, False, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True, False, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "            True,  True,  True,  True, False, False, False, False, False, False]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_std_mask(tgt_tensor, tokenizer.token_to_id('<pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "model_config ={\n",
    "    'd_model': 512, \n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 6, \n",
    "    'num_decoder_layers': 6,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1, \n",
    "    'src_vocab': tokenizer.get_vocab_size(), \n",
    "    'tgt_vocab': tokenizer.get_vocab_size()\n",
    "}\n",
    "d_model = 512  # Model dimension\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_encoder_layers = 6  # Number of encoder layers\n",
    "num_decoder_layers = 6  # Number of decoder layers\n",
    "d_ff = 2048  # Dimension of feedforward layers\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()  # Vocabulary size from your tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder, decoder, and the full model\n",
    "encoder = components.Encoder(num_encoder_layers, num_heads, d_model, d_ff, dropout)\n",
    "decoder = components.Decoder(num_decoder_layers, num_heads, d_model, d_ff, dropout)\n",
    "src_embed = nn.Sequential(nn.Embedding(vocab_size, d_model), components.PositionalEncoding(d_model, dropout))\n",
    "tgt_embed = nn.Sequential(nn.Embedding(vocab_size, d_model), components.PositionalEncoding(d_model, dropout))\n",
    "generator = components.Generator(d_model, vocab_size)\n",
    "\n",
    "# Initialize the EncoderDecoder model\n",
    "model = components.EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning rate scheduler, following `Attention is All You Need` for now. \n",
    "# lr = d_model ** (-0.5) * min(step_num ** (-0.5), step_num * warmup_steps ** (-1.5))\n",
    "d_model = 512\n",
    "warmup_steps = 4000\n",
    "\n",
    "def get_lr(step_num):\n",
    "    return d_model ** -0.5 * min(step_num ** -0.5, step_num * warmup_steps ** -1.5)\n",
    "\n",
    "\n",
    "# initialize optimizer, criterion\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_blocks): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (src_attn): MultiHeadAttention(\n",
       "          (query_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PositionwiseFFN(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layernorms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embedding(37000, 512)\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embedding(37000, 512)\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=37000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b59a2ce6f249dca2ceb3b90c9087cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 1:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 6.9416\n",
      "Epoch 1/3, Validation Loss: 6.8803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5557899dbbcb46c78a362ff43c90f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 2:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 5.2492\n",
      "Epoch 2/3, Validation Loss: 6.7635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7daacc75ed7442a9b68e004537d3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch: 3:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 4.9506\n",
      "Epoch 3/3, Validation Loss: 6.7014\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Number of epochs to train\n",
    "num_epochs = 3\n",
    "step_num = 0\n",
    "pad_token_id = tokenizer.token_to_id('<pad>')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dl, desc=f\"Training Epoch: {epoch + 1}\"):\n",
    "        step_num += 1\n",
    "\n",
    "        # adjust the learning rate according to the schedule\n",
    "        lr = get_lr(step_num)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device).unsqueeze(1).unsqueeze(2)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "\n",
    "        # shift the target token ids for the decoder input\n",
    "        tgt_input = labels[:, :-1]\n",
    "        tgt_y = labels[:, 1:]\n",
    "\n",
    "        # create the target mask (combining padding and look-ahead masks)\n",
    "        tgt_mask = make_std_mask(tgt_input, pad_token_id)\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src=input_ids, tgt=tgt_input, src_mask=attention_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_y.reshape(-1))\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_train_loss / len(train_dl)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device).unsqueeze(1).unsqueeze(2)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            tgt_input = labels[:, :-1]\n",
    "            tgt_y = labels[:, 1:]\n",
    "\n",
    "            tgt_mask = make_std_mask(tgt_input, pad_token_id)\n",
    "\n",
    "            logits = model(input_ids, tgt_input, attention_mask, tgt_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_y.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dl)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_tensor shape: torch.Size([1, 16])\n",
      "attention_mask shape: torch.Size([1, 1, 1, 16])\n",
      "End of sentence token encountered, stopping inference.\n",
      "German: <s>Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten</s>\n",
      "Actual translation: A Republican strategy to counter the re-election of Obama\n",
      "NMT: <s>It is therefore, as the same time, as the Treaty of the Treaty' s Party.</s>\n",
      "--------------------------------------------------\n",
      "src_tensor shape: torch.Size([1, 22])\n",
      "attention_mask shape: torch.Size([1, 1, 1, 22])\n",
      "End of sentence token encountered, stopping inference.\n",
      "German: <s>Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.</s>\n",
      "Actual translation: Republican leaders justified their policy by the need to combat electoral fraud.\n",
      "NMT: <s>The next item is the same time, the United States, the citizens' s own country.</s>\n",
      "--------------------------------------------------\n",
      "src_tensor shape: torch.Size([1, 42])\n",
      "attention_mask shape: torch.Size([1, 1, 1, 42])\n",
      "End of sentence token encountered, stopping inference.\n",
      "German: <s>Allerdings hält das Brennan Center letzteres für einen Mythos, indem es bekräftigt, dass der Wahlbetrug in den USA seltener ist als die Anzahl der vom Blitzschlag getöteten Menschen.</s>\n",
      "Actual translation: However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.\n",
      "NMT: <s>It is a very important to the European Public Prosecutor' s report, as a Europe, as a very much to the European Union.</s>\n",
      "--------------------------------------------------\n",
      "src_tensor shape: torch.Size([1, 25])\n",
      "attention_mask shape: torch.Size([1, 1, 1, 25])\n",
      "End of sentence token encountered, stopping inference.\n",
      "German: <s>Die Rechtsanwälte der Republikaner haben in 10 Jahren in den USA übrigens nur 300 Fälle von Wahlbetrug verzeichnet.</s>\n",
      "Actual translation: Indeed, Republican lawyers identified only 300 cases of electoral fraud in the United States in a decade.\n",
      "NMT: <s>The Commission' s report on the Treaty of the Treaty of the Treaty of the Treaty of the Treaty.</s>\n",
      "--------------------------------------------------\n",
      "src_tensor shape: torch.Size([1, 19])\n",
      "attention_mask shape: torch.Size([1, 1, 1, 19])\n",
      "End of sentence token encountered, stopping inference.\n",
      "German: <s>Eins ist sicher: diese neuen Bestimmungen werden sich negativ auf die Wahlbeteiligung auswirken.</s>\n",
      "Actual translation: One thing is certain: these new provisions will have a negative impact on voter turn-out.\n",
      "NMT: <s>It is therefore, however, the European Union' s own own country, and the environment.</s>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check on a few german sentences\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_examples = 5\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(tokenized_val[i])\n",
    "\n",
    "def decode_tokens(tokens, tokenizer):\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# perform inference\n",
    "with torch.no_grad():\n",
    "    for i, src in enumerate(examples):\n",
    "\n",
    "        # convert to tensor and move to device\n",
    "        src_tensor = torch.tensor(src['input_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask = (src_tensor != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        print(f\"src_tensor shape: {src_tensor.shape}\")\n",
    "        print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "\n",
    "        tgt_tensor = torch.tensor([tokenizer.token_to_id('<s>')]).unsqueeze(0).to(device)\n",
    "\n",
    "        for _ in range(100):  # limit the length of the generated sequence for now at least...\n",
    "            # create the tgt_mask\n",
    "            tgt_mask = make_std_mask(tgt_tensor, pad_token_id)\n",
    "\n",
    "            # run the model\n",
    "            logits = model(src_tensor, tgt_tensor, attention_mask, tgt_mask)\n",
    "\n",
    "            # get the predicted next_token\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1)\n",
    "            # print(f\"Next token predicted: {next_token.item()} (Token: {tokenizer.decode([next_token.item()])})\")\n",
    "\n",
    "            tgt_tensor = torch.cat([tgt_tensor, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # Check if the predicted token is </s>\n",
    "            if next_token.item() == tokenizer.token_to_id('</s>'):\n",
    "                print(\"End of sentence token encountered, stopping inference.\")\n",
    "                break\n",
    "\n",
    "        # decode the source and target\n",
    "        src_sentence = decode_tokens(src['input_ids'], tokenizer=tokenizer)\n",
    "        tgt_sentence = decode_tokens(tgt_tensor.squeeze().tolist(), tokenizer)\n",
    "        actual_tgt = src['translation']['en']\n",
    "\n",
    "        print(f\"German: {src_sentence}\")\n",
    "        print(f\"Actual translation: {actual_tgt}\")\n",
    "        print(f\"NMT: {tgt_sentence}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
